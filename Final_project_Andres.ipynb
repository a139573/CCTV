{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "901a80ce-40a3-48c9-b7a0-26478a34cf90",
   "metadata": {},
   "source": [
    "# Final Proyect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde496ca-06be-49ee-8802-030d73c335d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import os, cv2, re\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1350a66-07ac-4301-8151-14093cb89d00",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea95355-b14d-403c-a261-bbd2ed26e932",
   "metadata": {},
   "source": [
    "Los datos han sido descargados de https://universe.roboflow.com/rjacaac1/ua-detrac-dataset-10k/dataset/2\n",
    "\n",
    "Estos son archivos txt los cuales estos son los nombres de las columnas en el mismo orden en el que lo encontramos en los archivos\n",
    "\n",
    "[class_id, center_x, center_y, width, height]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05facb2-345d-40d3-ba27-0af079e7c19a",
   "metadata": {},
   "source": [
    "Ahora vamos a visualizar las imagenes que estan en formato jpg. \n",
    "\n",
    "- UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/images/\n",
    "- UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/valid/images/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20312ec2-adae-4c46-a5d2-a3333ba3873c",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab98255-7227-4218-b551-4575ef42ac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando train: 9316 im√°genes encontradas.\n",
      "Procesando valid: 500 im√°genes encontradas.\n",
      "\n",
      "--- Primeras 5 filas del DataFrame ---\n",
      "                                            image_id  \\\n",
      "0  MVI_20052_img00158_jpg.rf.50ed1502ff234681b511...   \n",
      "1  MVI_39271_img00494_jpg.rf.1184116be16d110fb166...   \n",
      "2  MVI_39361_img01720_jpg.rf.c1a0eb49147326a4b1bc...   \n",
      "3  MVI_39401_img00559_jpg.rf.73be5b65ab57747a13c3...   \n",
      "4  MVI_39771_img00025_jpg.rf.cc566f41b407cf2eb0ba...   \n",
      "\n",
      "                                          image_path  \\\n",
      "0  ./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.y...   \n",
      "1  ./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.y...   \n",
      "2  ./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.y...   \n",
      "3  ./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.y...   \n",
      "4  ./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.y...   \n",
      "\n",
      "                                          label_path    set  \n",
      "0  ./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.y...  train  \n",
      "1  ./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.y...  train  \n",
      "2  ./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.y...  train  \n",
      "3  ./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.y...  train  \n",
      "4  ./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.y...  train  \n",
      "\n",
      "--- Resumen del Dataset ---\n",
      "set\n",
      "train    9316\n",
      "valid     500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define la ruta ra√≠z donde est√°s ahora (el punto '.' significa carpeta actual)\n",
    "# Si ejecutas el script desde fuera, cambia '.' por la ruta completa\n",
    "dataset_path = './UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8' \n",
    "\n",
    "# Listas para almacenar la informaci√≥n\n",
    "data = []\n",
    "\n",
    "# Las carpetas que suele tener YOLOv8 (a veces 'test' no existe, por eso lo comprobaremos)\n",
    "# splits = ['train', 'valid', 'test']\n",
    "splits = ['train', 'valid'] # NO EXISTE TEST EN NUESTRA DESCARGA \n",
    "\n",
    "for split in splits:\n",
    "    # Construimos la ruta: ./train/images/*.jpg\n",
    "    search_path = os.path.join(dataset_path, split, 'images', '*.jpg')\n",
    "    images = glob.glob(search_path)\n",
    "    \n",
    "    # Si la carpeta no existe o est√° vac√≠a (ej. test), pasamos a la siguiente\n",
    "    if not images:\n",
    "        continue\n",
    "\n",
    "    print(f\"Procesando {split}: {len(images)} im√°genes encontradas.\")\n",
    "\n",
    "    for img_path in images:\n",
    "        # 1. Obtener ID de la imagen (nombre del archivo sin extensi√≥n)\n",
    "        filename = os.path.basename(img_path)\n",
    "        image_id = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # 2. Construir la ruta de la etiqueta correspondiente\n",
    "        # La l√≥gica de YOLO es cambiar /images/ por /labels/ y .jpg por .txt\n",
    "        label_path = img_path.replace('/images/', '/labels/').replace('.jpg', '.txt')\n",
    "        \n",
    "        # Opcional: Verificar si el txt existe (para evitar errores luego)\n",
    "        # if not os.path.exists(label_path):\n",
    "        #     label_path = None # O marcar como 'missing'\n",
    "            \n",
    "        data.append({\n",
    "            'image_id': image_id,\n",
    "            'image_path': img_path,   # Ruta de la imagen\n",
    "            'label_path': label_path, # Ruta del txt\n",
    "            'set': split            # 'train', 'valid' o 'test'\n",
    "        })\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Muestra las primeras filas\n",
    "print(\"\\n--- Primeras 5 filas del DataFrame ---\")\n",
    "print(df.head())\n",
    "\n",
    "# Muestra el conteo por set para verificar\n",
    "print(\"\\n--- Resumen del Dataset ---\")\n",
    "print(df['set'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd574a85-9a63-4dd0-9c53-2360f639bbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_id', 'image_path', 'label_path', 'set'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c94303",
   "metadata": {},
   "source": [
    "# Training of the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a443ee-745b-42f6-a4f5-df373f8d7bca",
   "metadata": {},
   "source": [
    "## BASELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79148cf8",
   "metadata": {},
   "source": [
    "Primero vamos a usar k-fold para el training y validar el modelo ya que tenemos muchas imagenes de train pero no tenemos muchas de validaci√≥n y las imagenes de train es un video continuo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ecf573-075b-45e5-a89f-f5d7800d309b",
   "metadata": {},
   "source": [
    "### K fold stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7d06c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {0: 'bus', 1: 'car', 2: 'truck', 3: 'van'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a090b365-3832-499b-be1b-e515b75b4c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GENERANDO 5 FOLDS EN 'folds' (MODO SEGURO) ---\n",
      "Buscando im√°genes en: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/images\n",
      "Im√°genes encontradas: 9316\n",
      "‚úÖ CORRECTO: Solo estamos usando el set de Entrenamiento.\n",
      "Calculando prioridades...\n",
      "--- Distribuci√≥n de Prioridades ---\n",
      "priority_group\n",
      "0    1350\n",
      "1    5063\n",
      "2    2903\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Guardando archivos en: /home/alumno/Desktop/datos/Computer_vision/final_project/folds\n",
      "\n",
      "‚úÖ ¬°Listo! Folds generados en la carpeta 'folds'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# ==========================================\n",
    "#        CONFIGURACI√ìN\n",
    "# ==========================================\n",
    "\n",
    "# 1. RUTA DEL DATASET\n",
    "# Ajusta esta ruta si es necesario, pero mant√©n la l√≥gica segura\n",
    "BASE_PROJECT_DIR = \"/home/alumno/Desktop/datos/Computer_vision/final_project\"\n",
    "dataset_path = os.path.join(BASE_PROJECT_DIR, \"UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8\")\n",
    "\n",
    "# 2. RUTA DE SALIDA (Exactamente como pediste: carpeta 'folds')\n",
    "# Se crear√° en el directorio donde est√©s ejecutando el script\n",
    "output_dir = 'folds'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "# 3. TUS CLASES\n",
    "class_names = {0: 'bus', 1: 'car', 2: 'truck', 3: 'van'}\n",
    "\n",
    "# 4. MAPA DE PRIORIDADES\n",
    "priority_map = {\n",
    "    0: 0,  # Bus    (Prioridad M√°xima)\n",
    "    2: 1,  # Truck  (Media)\n",
    "    3: 1,  # Van    (Media)\n",
    "    1: 2   # Car    (Baja)\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "#           L√ìGICA DEL PROGRAMA\n",
    "# ==========================================\n",
    "\n",
    "print(f\"--- GENERANDO {num_folds} FOLDS EN '{output_dir}' (MODO SEGURO) ---\")\n",
    "\n",
    "# --- 1. B√öSQUEDA BLINDADA (SOLO TRAIN) ---\n",
    "# Apuntamos directo a la carpeta train para evitar las de validaci√≥n\n",
    "train_imgs_dir = os.path.join(dataset_path, \"train/images\")\n",
    "print(f\"Buscando im√°genes en: {train_imgs_dir}\")\n",
    "\n",
    "# B√∫squeda directa\n",
    "images = glob.glob(os.path.join(train_imgs_dir, '*.jpg'))\n",
    "\n",
    "# Si est√°n en subcarpetas, buscamos recursivo SOLO dentro de train\n",
    "if len(images) == 0:\n",
    "    print(\"Buscando en subcarpetas de train...\")\n",
    "    images = glob.glob(os.path.join(train_imgs_dir, '**', '*.jpg'), recursive=True)\n",
    "\n",
    "print(f\"Im√°genes encontradas: {len(images)}\")\n",
    "\n",
    "# --- VERIFICACI√ìN DE SEGURIDAD ---\n",
    "# Si pasa de 9400, es que se han colado las de Test/Validaci√≥n.\n",
    "if len(images) > 9400:\n",
    "    print(\"‚ùå ALERTA: Demasiadas im√°genes. Se est√°n colando las de validaci√≥n.\")\n",
    "    print(\"El script se detiene por seguridad.\")\n",
    "    exit()\n",
    "elif len(images) == 0:\n",
    "    print(\"‚ùå ERROR: No se encontraron im√°genes.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"‚úÖ CORRECTO: Solo estamos usando el set de Entrenamiento.\")\n",
    "\n",
    "# --- 2. ASIGNACI√ìN DE PRIORIDADES ---\n",
    "data = []\n",
    "print(\"Calculando prioridades...\")\n",
    "\n",
    "for img_path in images:\n",
    "    label_path = img_path.replace('/images/', '/labels/').replace('.jpg', '.txt')\n",
    "    assigned_priority = 99 \n",
    "    \n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        if lines:\n",
    "            try:\n",
    "                classes_in_img = set([int(line.split()[0]) for line in lines])\n",
    "            except:\n",
    "                classes_in_img = set()\n",
    "            \n",
    "            best_prio = 99\n",
    "            for c in classes_in_img:\n",
    "                p = priority_map.get(c, 10) \n",
    "                if p < best_prio:\n",
    "                    best_prio = p\n",
    "            assigned_priority = best_prio\n",
    "        else:\n",
    "             assigned_priority = 99\n",
    "    else:\n",
    "        assigned_priority = 99\n",
    "\n",
    "    data.append({\n",
    "        'path': os.path.abspath(img_path), \n",
    "        'priority_group': assigned_priority\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"--- Distribuci√≥n de Prioridades ---\")\n",
    "print(df['priority_group'].value_counts().sort_index())\n",
    "\n",
    "# --- 3. GENERAR FOLDS EN LA CARPETA 'folds' ---\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "X = df['path']\n",
    "y = df['priority_group']\n",
    "\n",
    "print(f\"\\nGuardando archivos en: {os.path.abspath(output_dir)}\")\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    train_files = df.iloc[train_idx]['path'].tolist()\n",
    "    val_files = df.iloc[val_idx]['path'].tolist()\n",
    "    \n",
    "    # Nombres de archivo como t√∫ quer√≠as: folds/train_fold_0.txt\n",
    "    train_txt = os.path.abspath(os.path.join(output_dir, f'train_fold_{fold_idx}.txt'))\n",
    "    val_txt = os.path.abspath(os.path.join(output_dir, f'val_fold_{fold_idx}.txt'))\n",
    "    \n",
    "    with open(train_txt, 'w') as f:\n",
    "        f.write('\\n'.join(train_files))\n",
    "    with open(val_txt, 'w') as f:\n",
    "        f.write('\\n'.join(val_files))\n",
    "        \n",
    "    yaml_content = {\n",
    "        'path': dataset_path, \n",
    "        'train': train_txt,\n",
    "        'val': val_txt,\n",
    "        'names': class_names\n",
    "    }\n",
    "    \n",
    "    yaml_filename = os.path.join(output_dir, f'fold_{fold_idx}.yaml')\n",
    "    with open(yaml_filename, 'w') as f:\n",
    "        yaml.dump(yaml_content, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n‚úÖ ¬°Listo! Folds generados en la carpeta '{output_dir}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd99758-eeb8-490c-9cce-7364f144f6b8",
   "metadata": {},
   "source": [
    "En la siguiente celda vamos a realizar el k-fold stratified. Se utiliza el modelo ya pre-entrenado de yolov8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ce245-9fca-4a34-92fb-353151678669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affced7a-4de2-4abd-a465-aa90b17e2813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIANDO CROSS-VALIDATION K=5 (Deterministic) ---\n",
      "\n",
      "Training Fold 1/5...\n",
      "New https://pypi.org/project/ultralytics/8.4.4 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=folds/fold_0.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=fold_0, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/kfold, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_0, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 115.3¬±12.0 MB/s, size: 55.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 7452 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7452/7452 1.8Kit/s 4.1s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 136.6¬±20.8 MB/s, size: 65.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1864 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1864/1864 1.2Kit/s 1.6s0.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "Plotting labels to /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_0/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_0\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10      2.16G      1.025      1.296      1.003        122        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.0it/s 42.3s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 9.7it/s 6.1s0.1ss\n",
      "                   all       1864      17172      0.827      0.771      0.838      0.643\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10      2.73G      0.951     0.7953     0.9829        105        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.4it/s 41.0s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.5it/s 5.6s0.2s\n",
      "                   all       1864      17172      0.876      0.793      0.866      0.672\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10      2.74G     0.9181     0.6841     0.9762        114        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 12.1it/s 38.4s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.0it/s 5.4s0.1s\n",
      "                   all       1864      17172      0.886      0.811      0.894      0.711\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10      2.75G     0.8875     0.6206     0.9687        135        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.3it/s 41.1s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.3it/s 5.2s0.2s\n",
      "                   all       1864      17172      0.891      0.833      0.904      0.715\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10      2.75G     0.8575     0.5708     0.9559        106        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.1it/s 42.0s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.2it/s 5.8s0.1s\n",
      "                   all       1864      17172      0.907      0.853      0.926      0.754\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10      2.75G     0.8239     0.5307     0.9425         92        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.6it/s 40.1s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.0it/s 5.4s0.1s\n",
      "                   all       1864      17172      0.904      0.884       0.94      0.772\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10      2.75G     0.7993     0.5081     0.9341         87        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.3it/s 41.2s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.8it/s 5.5s0.1s\n",
      "                   all       1864      17172      0.921      0.873      0.946      0.781\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10      2.75G     0.7755     0.4776     0.9263         93        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.6it/s 40.2s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.3it/s 5.2s0.2s\n",
      "                   all       1864      17172      0.941      0.888      0.954      0.805\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10      2.75G     0.7558      0.452     0.9212        115        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 12.0it/s 39.0s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.2it/s 5.8s0.2s\n",
      "                   all       1864      17172       0.94      0.907      0.965       0.82\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10      2.75G       0.73     0.4315     0.9133         78        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.0it/s 42.3s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.5it/s 5.6s0.1s\n",
      "                   all       1864      17172      0.939      0.919      0.968      0.827\n",
      "\n",
      "10 epochs completed in 0.130 hours.\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_0/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_0/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_0/weights/best.pt...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 8.1it/s 7.3s<0.1s\n",
      "                   all       1864      17172      0.939      0.919      0.968      0.827\n",
      "                   bus        270        282      0.928      0.929      0.965       0.84\n",
      "                   car       1841      14104       0.95      0.924      0.979      0.795\n",
      "                 truck        806       1331      0.908      0.853      0.936      0.787\n",
      "                   van        765       1455      0.969       0.97      0.991      0.887\n",
      "Speed: 0.1ms preprocess, 0.3ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_0\u001b[0m\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.1 ms, read: 854.9¬±577.7 MB/s, size: 56.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache... 1864 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1864/1864 3.0Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 14.5it/s 8.1s0.1s\n",
      "                   all       1864      17172      0.939      0.919      0.968      0.828\n",
      "                   bus        270        282      0.929      0.929      0.965      0.842\n",
      "                   car       1841      14104       0.95      0.923      0.979      0.795\n",
      "                 truck        806       1331      0.909      0.853      0.936      0.787\n",
      "                   van        765       1455      0.969       0.97      0.991      0.888\n",
      "Speed: 0.6ms preprocess, 0.5ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val55\u001b[0m\n",
      "\n",
      "Training Fold 2/5...\n",
      "New https://pypi.org/project/ultralytics/8.4.4 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=folds/fold_1.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=fold_1, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/kfold, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_1, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 389.5¬±402.6 MB/s, size: 55.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 7453 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7453/7453 2.1Kit/s 3.6s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 389.8¬±381.3 MB/s, size: 61.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 1.4Kit/s 1.3s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "Plotting labels to /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_1/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_1\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10       2.2G      1.025      1.293      1.003        122        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 10.7it/s 43.4s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 9.8it/s 6.0s<0.1s\n",
      "                   all       1863      16920      0.834      0.753      0.833      0.626\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10      2.56G     0.9557     0.7928     0.9851        105        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 12.4it/s 37.6s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.4it/s 5.6s0.1s\n",
      "                   all       1863      16920      0.837      0.788      0.863      0.676\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10      2.56G     0.9215     0.6901     0.9785        104        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.5it/s 40.5s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.3it/s 5.2s0.1s\n",
      "                   all       1863      16920      0.855      0.815      0.873      0.675\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10      2.56G     0.8908     0.6205     0.9688        163        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.6it/s 40.1s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.3it/s 5.2s0.1s\n",
      "                   all       1863      16920      0.872      0.864      0.915      0.737\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10      2.56G     0.8523     0.5748     0.9561        121        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.8it/s 39.6s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.3it/s 5.7s0.1s\n",
      "                   all       1863      16920      0.876      0.881      0.928      0.748\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10      2.56G     0.8182     0.5306     0.9435        120        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 12.3it/s 37.8s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.6it/s 5.1s0.2s\n",
      "                   all       1863      16920      0.899       0.88      0.931       0.77\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10      2.56G      0.796     0.5035      0.933        104        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.5it/s 40.6s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.3it/s 5.2s0.2s\n",
      "                   all       1863      16920      0.912      0.879      0.943      0.775\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10      2.56G     0.7711     0.4736     0.9263         86        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 12.3it/s 38.0s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.9it/s 5.4s0.2s\n",
      "                   all       1863      16920      0.914      0.899      0.945       0.79\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10      2.56G     0.7536      0.452     0.9197        106        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 12.6it/s 37.1s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.7it/s 5.0s0.1s\n",
      "                   all       1863      16920      0.933      0.906      0.958      0.808\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10      2.56G     0.7353     0.4315     0.9141        110        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 12.4it/s 37.5s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.3it/s 5.2s0.2s\n",
      "                   all       1863      16920      0.928      0.925      0.961      0.815\n",
      "\n",
      "10 epochs completed in 0.125 hours.\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_1/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_1/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_1/weights/best.pt...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 8.4it/s 7.0s<0.2s\n",
      "                   all       1863      16920      0.928      0.926      0.961      0.815\n",
      "                   bus        270        288      0.953      0.922      0.954      0.824\n",
      "                   car       1830      13912      0.927       0.94      0.977      0.796\n",
      "                 truck        845       1349      0.873      0.864      0.922      0.757\n",
      "                   van        737       1371      0.959      0.977      0.991      0.883\n",
      "Speed: 0.1ms preprocess, 0.3ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_1\u001b[0m\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 307.3¬±226.3 MB/s, size: 51.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 3.4Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 15.1it/s 7.7s0.1s\n",
      "                   all       1863      16920      0.928      0.925      0.961      0.815\n",
      "                   bus        270        288      0.953      0.922      0.954      0.823\n",
      "                   car       1830      13912      0.927      0.939      0.977      0.797\n",
      "                 truck        845       1349      0.874      0.862      0.922      0.758\n",
      "                   van        737       1371      0.959      0.977      0.991      0.883\n",
      "Speed: 0.6ms preprocess, 0.5ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val56\u001b[0m\n",
      "\n",
      "Training Fold 3/5...\n",
      "New https://pypi.org/project/ultralytics/8.4.4 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=folds/fold_2.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=fold_2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/kfold, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 745.7¬±397.3 MB/s, size: 54.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 7453 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7453/7453 2.1Kit/s 3.6s0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 585.5¬±354.8 MB/s, size: 61.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 1.4Kit/s 1.3s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "Plotting labels to /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10      2.16G       1.03      1.292      1.008        136        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.5it/s 40.6s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.4it/s 5.7s0.1s\n",
      "                   all       1863      17283      0.759      0.724      0.778      0.598\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10      2.18G     0.9603     0.7976     0.9879         95        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.9it/s 39.1s0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.5it/s 5.6s0.2s\n",
      "                   all       1863      17283      0.816      0.774      0.846      0.648\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10      2.18G     0.9282     0.6873     0.9808        108        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.4it/s 40.9s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 9.7it/s 6.1s0.1ss\n",
      "                   all       1863      17283      0.886      0.812      0.886      0.698\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10      2.18G     0.8921     0.6216      0.969        171        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.7it/s 40.0s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.9it/s 5.4s0.2s\n",
      "                   all       1863      17283      0.866      0.819      0.888      0.713\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10      2.18G     0.8586     0.5749     0.9557        132        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.2it/s 41.7s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.1it/s 5.3s0.1s\n",
      "                   all       1863      17283      0.902      0.857      0.931      0.765\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10      2.18G     0.8241     0.5341     0.9453        115        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.6it/s 40.2s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.8it/s 5.5s0.1s\n",
      "                   all       1863      17283      0.907      0.879       0.94      0.776\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10      2.18G     0.7985     0.5045     0.9343        113        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.4it/s 40.9s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.3it/s 5.2s0.1s\n",
      "                   all       1863      17283      0.915      0.886      0.944      0.788\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10      2.18G     0.7785     0.4773     0.9297         80        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.3it/s 41.2s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.0it/s 5.4s0.2s\n",
      "                   all       1863      17283      0.913      0.896       0.95      0.802\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10      2.18G     0.7567     0.4541     0.9205        141        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.7it/s 39.9s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.8it/s 5.5s0.1s\n",
      "                   all       1863      17283      0.922      0.906       0.95      0.808\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10      2.18G     0.7368     0.4325     0.9131        104        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.0it/s 42.5s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.5it/s 5.1s0.1s\n",
      "                   all       1863      17283      0.936      0.908      0.962      0.823\n",
      "\n",
      "10 epochs completed in 0.130 hours.\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_2/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_2/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_2/weights/best.pt...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 8.4it/s 7.0s<0.1s\n",
      "                   all       1863      17283      0.936      0.908      0.962      0.823\n",
      "                   bus        270        289      0.932      0.903      0.959      0.839\n",
      "                   car       1843      14182      0.951      0.914      0.975      0.793\n",
      "                 truck        820       1291      0.899      0.841      0.925      0.772\n",
      "                   van        779       1521       0.96      0.974       0.99      0.888\n",
      "Speed: 0.1ms preprocess, 0.3ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_2\u001b[0m\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1054.5¬±328.9 MB/s, size: 57.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 3.1Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 13.9it/s 8.4s0.1s\n",
      "                   all       1863      17283      0.936      0.908      0.962      0.823\n",
      "                   bus        270        289      0.932      0.903      0.959      0.838\n",
      "                   car       1843      14182      0.951      0.914      0.975      0.794\n",
      "                 truck        820       1291      0.899      0.842      0.925      0.772\n",
      "                   van        779       1521       0.96      0.974       0.99      0.888\n",
      "Speed: 0.7ms preprocess, 0.5ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val57\u001b[0m\n",
      "\n",
      "Training Fold 4/5...\n",
      "New https://pypi.org/project/ultralytics/8.4.4 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=folds/fold_3.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=fold_3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/kfold, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.1 ms, read: 514.9¬±552.5 MB/s, size: 55.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 7453 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7453/7453 2.2Kit/s 3.4s0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 258.9¬±181.8 MB/s, size: 62.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 1.4Kit/s 1.3s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "Plotting labels to /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_3\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10      2.18G      1.029      1.296      1.006        109        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.0it/s 42.5s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 9.8it/s 6.0s<0.2s\n",
      "                   all       1863      17211      0.792      0.731      0.803      0.616\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10      2.38G     0.9561     0.7956     0.9852        118        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.6it/s 40.2s0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.6it/s 5.5s0.1s\n",
      "                   all       1863      17211      0.856      0.794       0.86      0.667\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10      2.38G      0.927     0.6898     0.9799        109        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 10.9it/s 42.6s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.1it/s 5.3s0.2s\n",
      "                   all       1863      17211      0.863      0.819      0.881       0.69\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10      2.38G     0.8876     0.6206     0.9682        148        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.6it/s 40.1s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.0it/s 5.4s0.2s\n",
      "                   all       1863      17211      0.859      0.828      0.898      0.719\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10      2.38G     0.8535     0.5729     0.9554        127        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.6it/s 40.0s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.5it/s 5.1s0.2s\n",
      "                   all       1863      17211      0.906      0.862       0.93      0.751\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10      2.38G     0.8222     0.5321     0.9443        110        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.2it/s 41.8s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.3it/s 5.7s.2ss\n",
      "                   all       1863      17211        0.9       0.88      0.936      0.766\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10      2.38G     0.7983     0.5031     0.9339         93        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.4it/s 40.7s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.7it/s 5.5s0.2s\n",
      "                   all       1863      17211       0.91      0.898      0.949       0.79\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10      2.38G     0.7742     0.4757     0.9274         88        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 12.0it/s 38.9s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.3it/s 5.2s0.2s\n",
      "                   all       1863      17211      0.919      0.894      0.952      0.796\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10      2.38G     0.7533     0.4525     0.9202        113        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 10.9it/s 42.8s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 11.1it/s 5.3s0.1s\n",
      "                   all       1863      17211      0.932      0.888      0.956      0.798\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10      2.38G     0.7366     0.4336     0.9128        108        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.5it/s 40.5s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.8it/s 5.5s.1ss\n",
      "                   all       1863      17211      0.936      0.917      0.965      0.819\n",
      "\n",
      "10 epochs completed in 0.130 hours.\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_3/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_3/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_3/weights/best.pt...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 8.1it/s 7.3s0.1ss\n",
      "                   all       1863      17211      0.936      0.917      0.965      0.818\n",
      "                   bus        270        292      0.961      0.917      0.969      0.832\n",
      "                   car       1837      14153      0.936      0.934      0.977      0.796\n",
      "                 truck        831       1342      0.897      0.847      0.928       0.77\n",
      "                   van        741       1424      0.951       0.97      0.987      0.876\n",
      "Speed: 0.1ms preprocess, 0.3ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_3\u001b[0m\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.1 ms, read: 415.9¬±292.3 MB/s, size: 52.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 3.1Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 14.1it/s 8.3s0.1s\n",
      "                   all       1863      17211      0.936      0.916      0.965      0.819\n",
      "                   bus        270        292      0.961      0.916      0.969      0.833\n",
      "                   car       1837      14153      0.936      0.934      0.977      0.796\n",
      "                 truck        831       1342      0.898      0.846      0.928       0.77\n",
      "                   van        741       1424      0.951      0.969      0.987      0.877\n",
      "Speed: 0.7ms preprocess, 0.6ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val58\u001b[0m\n",
      "\n",
      "Training Fold 5/5...\n",
      "New https://pypi.org/project/ultralytics/8.4.4 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=folds/fold_4.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=fold_4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/kfold, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 585.4¬±538.7 MB/s, size: 56.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 7453 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7453/7453 2.2Kit/s 3.4s0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 624.9¬±290.2 MB/s, size: 55.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 1.4Kit/s 1.3s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "Plotting labels to /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_4\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10      2.14G      1.031      1.298      1.006        115        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 10.6it/s 43.8s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 9.5it/s 6.2s<0.2s\n",
      "                   all       1863      17157      0.802      0.712      0.798      0.617\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10      2.34G     0.9566     0.7954     0.9853        110        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 10.9it/s 42.8s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.3it/s 5.7s0.1s\n",
      "                   all       1863      17157      0.845      0.783      0.859      0.666\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10      2.34G     0.9268     0.6841     0.9805        108        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.0it/s 42.5s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.3it/s 5.7s0.1s\n",
      "                   all       1863      17157      0.835      0.767      0.842      0.666\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10      2.34G     0.8902      0.623     0.9681        169        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 8.2it/s 56.9s<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.8it/s 8.7s0.3s\n",
      "                   all       1863      17157      0.876      0.842      0.909      0.737\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10      2.34G      0.854      0.573     0.9556        139        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 7.3it/s 1:044<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.5it/s 5.6s0.1s\n",
      "                   all       1863      17157      0.882      0.879       0.93      0.754\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10      2.34G       0.82      0.532     0.9431        121        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.2it/s 41.6s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.5it/s 5.6s0.2s\n",
      "                   all       1863      17157      0.916      0.859      0.938      0.776\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10      2.34G      0.799     0.5031     0.9349        111        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.2it/s 41.5s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.0it/s 5.9s0.2s\n",
      "                   all       1863      17157      0.915      0.883      0.939      0.781\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10      2.34G     0.7717     0.4756     0.9265        103        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 11.2it/s 41.4s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 10.4it/s 5.7s.2ss\n",
      "                   all       1863      17157      0.924      0.893      0.953      0.801\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10      2.34G     0.7506     0.4535     0.9175        129        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 9.0it/s 51.7ss<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.3it/s 8.1s0.1s\n",
      "                   all       1863      17157      0.928      0.896      0.959       0.81\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10      2.34G     0.7328     0.4295     0.9102        110        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 8.2it/s 56.8s<0.3ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.6it/s 7.7s0.1s\n",
      "                   all       1863      17157      0.918      0.926      0.963      0.819\n",
      "\n",
      "10 epochs completed in 0.153 hours.\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_4/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_4/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_4/weights/best.pt...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.3it/s 9.3s0.1s\n",
      "                   all       1863      17157      0.918      0.926      0.963      0.818\n",
      "                   bus        270        287      0.929      0.912      0.953      0.822\n",
      "                   car       1848      14142      0.928      0.943      0.978      0.797\n",
      "                 truck        835       1356      0.856      0.884       0.93      0.779\n",
      "                   van        751       1372      0.959      0.965      0.989      0.876\n",
      "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_4\u001b[0m\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 703.2¬±460.2 MB/s, size: 59.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 2.4Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 12.0it/s 9.8s0.1s\n",
      "                   all       1863      17157      0.918      0.926      0.963      0.819\n",
      "                   bus        270        287      0.929      0.912      0.953      0.823\n",
      "                   car       1848      14142      0.928      0.943      0.978      0.798\n",
      "                 truck        835       1356      0.857      0.883       0.93      0.778\n",
      "                   van        751       1372      0.959      0.965      0.989      0.876\n",
      "Speed: 0.8ms preprocess, 0.5ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val59\u001b[0m\n",
      "\n",
      "========================================\n",
      "RESUMEN K-FOLD (Avg over 5 folds)\n",
      "========================================\n",
      "Fold 0: mAP@50 = 0.9677 | mAP@50-95 = 0.8280\n",
      "Fold 1: mAP@50 = 0.9609 | mAP@50-95 = 0.8155\n",
      "Fold 2: mAP@50 = 0.9624 | mAP@50-95 = 0.8229\n",
      "Fold 3: mAP@50 = 0.9654 | mAP@50-95 = 0.8191\n",
      "Fold 4: mAP@50 = 0.9626 | mAP@50-95 = 0.8185\n",
      "----------------------------------------\n",
      "Promedio mAP@50:    0.9638\n",
      "Promedio mAP@50-95: 0.8208\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- CONFIGURACI√ìN ---\n",
    "num_folds = 5\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "img_size = 640\n",
    "model_name = 'yolov8n.pt' \n",
    "\n",
    "# Listas para guardar resultados\n",
    "map50_results = []\n",
    "map50_95_results = []\n",
    "\n",
    "print(f\"--- INICIANDO CROSS-VALIDATION K={num_folds} (Deterministic) ---\")\n",
    "\n",
    "for k in range(num_folds):\n",
    "    print(f\"\\nTraining Fold {k+1}/{num_folds}...\")\n",
    "    \n",
    "    # Cargar modelo limpio\n",
    "    model = YOLO(model_name)\n",
    "    \n",
    "    results = model.train(\n",
    "        data=f'folds/fold_{k}.yaml', \n",
    "        epochs=epochs,\n",
    "        imgsz=img_size,\n",
    "        batch=batch_size,\n",
    "        project='runs/kfold',\n",
    "        name=f'fold_{k}',\n",
    "        patience=10,\n",
    "        verbose=True,\n",
    "        exist_ok=True,\n",
    "        \n",
    "        # --- AJUSTES DE REPRODUCIBILIDAD ---\n",
    "        seed=42,             \n",
    "        deterministic=True,  # Fuerza operaciones deterministas en CUDA (un poco m√°s lento, pero exacto)\n",
    "        workers=4            # N√∫mero de hilos para cargar datos\n",
    "    )\n",
    "    \n",
    "    # Validaci√≥n\n",
    "    metrics = model.val()\n",
    "    \n",
    "    map50_results.append(metrics.box.map50)\n",
    "    map50_95_results.append(metrics.box.map)\n",
    "\n",
    "# --- INFORME FINAL ---\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"RESUMEN K-FOLD (Avg over {num_folds} folds)\")\n",
    "print(\"=\"*40)\n",
    "for i in range(num_folds):\n",
    "    print(f\"Fold {i}: mAP@50 = {map50_results[i]:.4f} | mAP@50-95 = {map50_95_results[i]:.4f}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Promedio mAP@50:    {np.mean(map50_results):.4f}\")\n",
    "print(f\"Promedio mAP@50-95: {np.mean(map50_95_results):.4f}\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "518f1f54-e6fe-4ba7-8fa8-708b30ee38c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RECUPERANDO RESULTADOS DE runs/kfold ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x600 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RESUMEN FINAL (Extra√≠do de results.csv)\n",
      "==================================================\n",
      "          Precision  Recall  mAP@50  mAP@50-95\n",
      "Fold                                          \n",
      "0            0.9392  0.9187  0.9677     0.8273\n",
      "1            0.9278  0.9255  0.9610     0.8146\n",
      "2            0.9356  0.9083  0.9624     0.8231\n",
      "3            0.9364  0.9168  0.9654     0.8187\n",
      "4            0.9179  0.9263  0.9626     0.8189\n",
      "PROMEDIO     0.9314  0.9191  0.9638     0.8205\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Ajusta esto si cambiaste la carpeta 'runs/kfold' en tu c√≥digo\n",
    "project_path = 'runs/kfold' \n",
    "num_folds = 5\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "print(f\"--- RECUPERANDO RESULTADOS DE {project_path} ---\")\n",
    "\n",
    "for k in range(num_folds):\n",
    "    fold_dir = os.path.join(project_path, f'fold_{k}')\n",
    "    \n",
    "    # 1. Recuperar M√©tricas del archivo results.csv\n",
    "    # YOLO guarda un historial epoch a epoch. Leemos la √∫ltima fila (o la mejor).\n",
    "    csv_file = os.path.join(fold_dir, 'results.csv')\n",
    "    \n",
    "    if os.path.exists(csv_file):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # Limpiamos espacios en los nombres de columnas\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # Buscamos la fila donde 'metrics/mAP50(B)' es m√°ximo\n",
    "        best_epoch = df.loc[df['metrics/mAP50(B)'].idxmax()]\n",
    "\n",
    "        #Aqu√≠ se coge las metricas con (B) porque yolo indica que estas m√©tricas son de la box\n",
    "        results_summary.append({\n",
    "            'Fold': k,\n",
    "            'Precision': best_epoch['metrics/precision(B)'],\n",
    "            'Recall': best_epoch['metrics/recall(B)'],\n",
    "            'mAP@50': best_epoch['metrics/mAP50(B)'],\n",
    "            'mAP@50-95': best_epoch['metrics/mAP50-95(B)']\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Advertencia: No se encontr√≥ results.csv para el Fold {k}\")\n",
    "\n",
    "    # 2. Visualizar Matriz de Confusi√≥n\n",
    "    # Buscamos la normalizada primero, si no la simple\n",
    "    matrix_path = os.path.join(fold_dir, 'confusion_matrix_normalized.png')\n",
    "    if not os.path.exists(matrix_path):\n",
    "        matrix_path = os.path.join(fold_dir, 'confusion_matrix.png')\n",
    "        \n",
    "    plt.subplot(1, 5, k+1)\n",
    "    if os.path.exists(matrix_path):\n",
    "        img = plt.imread(matrix_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Fold {k}\")\n",
    "        plt.axis('off')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, \"No Matriz\", ha='center')\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Mostrar Tabla de Resultados\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESUMEN FINAL (Extra√≠do de results.csv)\")\n",
    "print(\"=\"*50)\n",
    "if results_summary:\n",
    "    df_res = pd.DataFrame(results_summary)\n",
    "    df_res.set_index('Fold', inplace=True)\n",
    "    \n",
    "    # A√±adir promedio\n",
    "    df_res.loc['PROMEDIO'] = df_res.mean()\n",
    "    \n",
    "    print(df_res.round(4))\n",
    "else:\n",
    "    print(\"No se encontraron resultados para mostrar.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f6b1ccb-6e58-4b4f-befb-be55646e7150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GENERANDO MATRICES FALTANTES DESDE runs/kfold ---\n",
      "\n",
      "Processing Fold 0 (Generando gr√°ficos)...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 424.6¬±471.5 MB/s, size: 56.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1864 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1864/1864 2.0Kit/s 0.9s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 13.3it/s 8.8s0.1s\n",
      "                   all       1864      17172      0.939      0.919      0.968      0.828\n",
      "Speed: 0.8ms preprocess, 0.5ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_0_val\u001b[0m\n",
      "\n",
      "Processing Fold 1 (Generando gr√°ficos)...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 555.7¬±377.1 MB/s, size: 47.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 2.3Kit/s 0.8s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 13.6it/s 8.6s0.2s\n",
      "                   all       1863      16920      0.928      0.925      0.961      0.815\n",
      "Speed: 0.7ms preprocess, 0.4ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_1_val\u001b[0m\n",
      "\n",
      "Processing Fold 2 (Generando gr√°ficos)...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 898.8¬±279.9 MB/s, size: 48.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 1.9Kit/s 1.0s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 13.3it/s 8.8s0.1s\n",
      "                   all       1863      17283      0.936      0.908      0.962      0.823\n",
      "Speed: 0.7ms preprocess, 0.5ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_2_val\u001b[0m\n",
      "\n",
      "Processing Fold 3 (Generando gr√°ficos)...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 186.2¬±32.7 MB/s, size: 55.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 1.9Kit/s 1.0s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 13.8it/s 8.5s0.1s\n",
      "                   all       1863      17211      0.936      0.916      0.965      0.819\n",
      "Speed: 0.7ms preprocess, 0.5ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_3_val\u001b[0m\n",
      "\n",
      "Processing Fold 4 (Generando gr√°ficos)...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 541.4¬±274.7 MB/s, size: 50.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 2.4Kit/s 0.8s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 13.9it/s 8.4s0.1s\n",
      "                   all       1863      17157      0.918      0.926      0.963      0.819\n",
      "Speed: 0.7ms preprocess, 0.5ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold/fold_4_val\u001b[0m\n",
      "\n",
      "==================================================\n",
      "MATRICES DE CONFUSI√ìN GENERADAS\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Configuraci√≥n\n",
    "num_folds = 5\n",
    "project_path = 'runs/kfold' \n",
    "\n",
    "# Lista para guardar las rutas de las nuevas matrices\n",
    "matrix_paths = []\n",
    "\n",
    "print(f\"--- GENERANDO MATRICES FALTANTES DESDE {project_path} ---\")\n",
    "\n",
    "for k in range(num_folds):\n",
    "    # 1. Buscamos los pesos del modelo que ya entrenaste\n",
    "    weights_path = os.path.join(project_path, f'fold_{k}', 'weights', 'best.pt')\n",
    "    \n",
    "    if os.path.exists(weights_path):\n",
    "        print(f\"\\nProcessing Fold {k} (Generando gr√°ficos)...\")\n",
    "        \n",
    "        # Cargar el modelo ya entrenado\n",
    "        model = YOLO(weights_path)\n",
    "        \n",
    "        # 2. Ejecutar validaci√≥n expl√≠cita para crear los gr√°ficos\n",
    "        # Guardaremos esto en una carpeta nueva 'fold_X_val' para no mezclar\n",
    "        model.val(\n",
    "            data=f'folds/fold_{k}.yaml',\n",
    "            project=project_path,\n",
    "            name=f'fold_{k}_val', # Carpeta nueva: runs/kfold/fold_0_val\n",
    "            plots=True,           # FUERZA la creaci√≥n de matrices\n",
    "            exist_ok=True,         # Sobrescribe si ya lo intentaste\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Guardamos la ruta donde deber√≠a haber aparecido la matriz\n",
    "        # YOLOv8 a veces la llama 'confusion_matrix_normalized.png' o sin normalizar\n",
    "        path_norm = os.path.join(project_path, f'fold_{k}_val', 'confusion_matrix_normalized.png')\n",
    "        path_raw = os.path.join(project_path, f'fold_{k}_val', 'confusion_matrix.png')\n",
    "        \n",
    "        if os.path.exists(path_norm):\n",
    "            matrix_paths.append(path_norm)\n",
    "        elif os.path.exists(path_raw):\n",
    "            matrix_paths.append(path_raw)\n",
    "        else:\n",
    "            matrix_paths.append(None)\n",
    "            \n",
    "    else:\n",
    "        print(f\"Error: No se encontraron pesos para el Fold {k} en {weights_path}\")\n",
    "        matrix_paths.append(None)\n",
    "\n",
    "# --- VISUALIZAR LAS MATRICES ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MATRICES DE CONFUSI√ìN GENERADAS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for k, img_path in enumerate(matrix_paths):\n",
    "    plt.subplot(1, 5, k+1)\n",
    "    if img_path and os.path.exists(img_path):\n",
    "        img = plt.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Fold {k}\")\n",
    "        plt.axis('off')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, \"No Image Found\", ha='center')\n",
    "        plt.title(f\"Fold {k} (Error)\")\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c2087c-f273-48b8-b3f3-30fc3e145ac9",
   "metadata": {},
   "source": [
    "#### Matriz de confusi√≥n global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bb0a77e-8551-42eb-87a9-8afcd3516483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CALCULANDO MATRIZ DE CONFUSI√ìN GLOBAL (K=5) ---\n",
      "Buscando modelos en: runs/kfold\n",
      "\n",
      ">>> Procesando Fold 0...\n",
      "   Validando con: folds/fold_0.yaml\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 439.6¬±260.1 MB/s, size: 61.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1864 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1864/1864 2.0Kit/s 0.9s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 11.4it/s 10.3s0.1s\n",
      "                   all       1864      17172      0.939      0.919      0.968      0.828\n",
      "Speed: 0.9ms preprocess, 0.6ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val65\u001b[0m\n",
      "   mAP50-95 obtenido: 0.8280\n",
      "\n",
      ">>> Procesando Fold 1...\n",
      "   Validando con: folds/fold_1.yaml\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.0 ms, read: 299.4¬±338.8 MB/s, size: 51.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 1.9Kit/s 1.0s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 11.2it/s 10.5s0.1s\n",
      "                   all       1863      16920      0.928      0.925      0.961      0.815\n",
      "Speed: 1.0ms preprocess, 0.6ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val66\u001b[0m\n",
      "   mAP50-95 obtenido: 0.8155\n",
      "\n",
      ">>> Procesando Fold 2...\n",
      "   Validando con: folds/fold_2.yaml\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.1 ms, read: 446.8¬±426.9 MB/s, size: 49.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 2.0Kit/s 0.9s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 11.9it/s 9.8s0.1s\n",
      "                   all       1863      17283      0.936      0.908      0.962      0.823\n",
      "Speed: 0.9ms preprocess, 0.6ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val67\u001b[0m\n",
      "   mAP50-95 obtenido: 0.8229\n",
      "\n",
      ">>> Procesando Fold 3...\n",
      "   Validando con: folds/fold_3.yaml\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 734.7¬±669.0 MB/s, size: 60.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 1.9Kit/s 1.0s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 11.3it/s 10.3s0.1s\n",
      "                   all       1863      17211      0.936      0.916      0.965      0.819\n",
      "Speed: 0.9ms preprocess, 0.6ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val68\u001b[0m\n",
      "   mAP50-95 obtenido: 0.8191\n",
      "\n",
      ">>> Procesando Fold 4...\n",
      "   Validando con: folds/fold_4.yaml\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 442.4¬±374.9 MB/s, size: 54.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 1.8Kit/s 1.0s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 13.7it/s 8.5s0.1s\n",
      "                   all       1863      17157      0.918      0.926      0.963      0.819\n",
      "Speed: 0.7ms preprocess, 0.4ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val69\u001b[0m\n",
      "   mAP50-95 obtenido: 0.8185\n",
      "\n",
      "Generando Gr√°fico Global...\n",
      "Se ha guardado la matriz normalizada transpuesta en:\n",
      "./normalized matrix k-fold.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Conteos Absolutos Totales ---\n",
      "[[       1324           4          12          11         169]\n",
      " [         23       67629         786          16        9646]\n",
      " [         36         505        5697          23        1608]\n",
      " [         16          25          18        6992         558]\n",
      " [         39        2330         156         101           0]]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# --- CONFIGURACI√ìN ---\n",
    "num_folds = 5\n",
    "project_path = 'runs/kfold'\n",
    "class_names = {0: 'bus', 1: 'car', 2: 'truck', 3: 'van'} \n",
    "# formato_columnas--> Sirve para hacer que las columnas de la matriz de normalizaci√≥n sumen 1 (Mismo formato que sckitic-learn)\n",
    "formato_columnas = True\n",
    "save_matrix = True\n",
    "path_matrix = \"./normalized matrix k-fold.png\"\n",
    "total_matrix = None \n",
    "\n",
    "print(f\"--- CALCULANDO MATRIZ DE CONFUSI√ìN GLOBAL (K={num_folds}) ---\")\n",
    "print(f\"Buscando modelos en: {project_path}\")\n",
    "\n",
    "for k in range(num_folds):\n",
    "    print(f\"\\n>>> Procesando Fold {k}...\")\n",
    "    \n",
    "    # 1. Verificar que el archivo de pesos existe\n",
    "    weights_path = os.path.join(project_path, f'fold_{k}', 'weights', 'best.pt')\n",
    "    \n",
    "    if not os.path.exists(weights_path):\n",
    "        print(f\"‚ùå ERROR: No se encuentra el archivo: {weights_path}\")\n",
    "        print(\"   ¬øSeguro que el entrenamiento termin√≥ y guard√≥ los pesos?\")\n",
    "        continue # Saltamos este fold si no hay modelo\n",
    "        \n",
    "    model = YOLO(weights_path)\n",
    "    \n",
    "    # 2. Validar CON plots=True para forzar el c√°lculo de la matriz\n",
    "    print(f\"   Validando con: folds/fold_{k}.yaml\")\n",
    "    metrics = model.val(\n",
    "        data=f'folds/fold_{k}.yaml', \n",
    "        plots=True,    \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Check de seguridad: ¬øEl modelo est√° detectando algo?\n",
    "    print(f\"   mAP50-95 obtenido: {metrics.box.map:.4f}\")\n",
    "    \n",
    "    if metrics.box.map == 0:\n",
    "        print(\"   ‚ö†Ô∏è CUIDADO: El mAP es 0. El modelo no detecta nada o no encuentra etiquetas.\")\n",
    "    \n",
    "    # 3. Extraer la matriz\n",
    "    current_matrix = metrics.confusion_matrix.matrix\n",
    "    \n",
    "    # 4. Sumar\n",
    "    if total_matrix is None:\n",
    "        total_matrix = current_matrix\n",
    "    else:\n",
    "        if total_matrix.shape == current_matrix.shape:\n",
    "            total_matrix += current_matrix\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Error de dimensiones en Fold {k}. Saltando.\")\n",
    "\n",
    "# --- VISUALIZACI√ìN ---\n",
    "if total_matrix is not None and total_matrix.sum() > 0:\n",
    "    print(\"\\nGenerando Gr√°fico Global...\")\n",
    "    \n",
    "    # Etiquetas + Background\n",
    "    labels = list(class_names.values()) + ['background']\n",
    "    \n",
    "    # Normalizaci√≥n segura\n",
    "    row_sums = total_matrix.sum(axis=1, keepdims=True)\n",
    "    # Evitar divisi√≥n por cero donde no hay datos\n",
    "    row_sums[row_sums == 0] = 1 \n",
    "    norm_matrix = total_matrix / row_sums\n",
    "\n",
    "    #Transponemos la matriz normalizada para que nos salgan que la suma de las columnas es igual a 1\n",
    "    norm_matrix_transpuesta = norm_matrix.T\n",
    "    \n",
    "    if formato_columnas: \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(norm_matrix_transpuesta, \n",
    "                    annot=True, \n",
    "                    fmt='.2f', \n",
    "                    cmap='Blues',\n",
    "                    xticklabels=labels, \n",
    "                    yticklabels=labels)\n",
    "    \n",
    "        plt.title(f'Matriz de Confusi√≥n Global (K-Fold Acumulado)\\nTotal Muestras: {int(total_matrix.sum())}')\n",
    "        plt.xlabel('PREDICTIONS')\n",
    "        plt.ylabel('TRUTH VALUES')\n",
    "        if save_matrix and path_matrix != None :\n",
    "            print(f\"Se ha guardado la matriz normalizada transpuesta en:\\n{path_matrix}\")\n",
    "            plt.savefig(path_matrix,dpi = 300)\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(norm_matrix,\n",
    "                    annot=True, \n",
    "                    fmt='.2f', \n",
    "                    cmap='Blues',\n",
    "                    xticklabels=labels, \n",
    "                    yticklabels=labels)\n",
    "    \n",
    "        plt.title(f'Matriz de Confusi√≥n Global (K-Fold Acumulado)\\nTotal Muestras: {int(total_matrix.sum())}')\n",
    "        plt.xlabel('TRUTH VALUES')\n",
    "        plt.ylabel('PREDICTIONS')\n",
    "        if save_matrix and path_matrix != None :\n",
    "            print(f\"Se ha guardado la matriz normalizada en:\\n {path_matrix}\")\n",
    "            plt.savefig(path_matrix,dpi = 300)\n",
    "        plt.show()\n",
    "    print(\"\\n--- Conteos Absolutos Totales ---\")\n",
    "    print(total_matrix)\n",
    "else:\n",
    "    print(\"\\n‚ùå ERROR FINAL: La matriz total es 0 o vac√≠a.\")\n",
    "    print(\"Revisa que los archivos .txt de las etiquetas est√©n en la ruta correcta definida en los YAML.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac853b1a-e29c-4d1a-9272-7eb2e44618ed",
   "metadata": {},
   "source": [
    "Los resultados obtenidos son muy buenos ya que vemos que los valores principales estan en la diagonal principal, con un 74 % el menor valor de las 4 clases. El background da valores altos en todas las clases debido a que muchos de estos ejemplos son debidos a que no estan etiquetados los veh√≠culos. Sin embargo, los modelos utilizados consiguen detectarlos. Por lo tanto, no nos vamos a preocupar por estos ejemplos. Si no tenemos en cuenta el background la clase que m√°s se confunde son los truck ( Que contiene a camiones y camionetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446f20d-146d-4c4a-9e6c-198cd7c3eb65",
   "metadata": {},
   "source": [
    "## Pruebas con transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ccaa3c-778d-449a-ae26-8a44d827c18a",
   "metadata": {},
   "source": [
    "## GRID DISTORTION AND ELASTIC DISTORTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574c49a-3608-4ee2-a2ef-bc221892a2e5",
   "metadata": {},
   "source": [
    "Hay que instalar la libreria albumentations. Esta libreria la utiliza YOLO para poder aplicar data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fff6117-406a-481b-b530-094b1d27b2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.7 environment at: /home/alumno/py313ml/.venv\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1malbumentations\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      "\u001b[2mUsing Python 3.13.7 environment at: /home/alumno/py313ml/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m17 packages\u001b[0m \u001b[2min 129ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m.1                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1malbumentations\u001b[0m\u001b[2m==1.3.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# EJECUTA ESTO Y LUEGO REINICIA EL KERNEL\n",
    "# !uv pip uninstall albumentations\n",
    "# !uv pip install albumentations==1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c55c6607-e207-4f78-aca9-5223e89c183f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versi√≥n de Albumentations cargada: 2.0.8\n"
     ]
    }
   ],
   "source": [
    "import albumentations\n",
    "print(f\"Versi√≥n de Albumentations cargada: {albumentations.__version__}\")\n",
    "# Debe imprimir: 1.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376abbb8-67ac-4c53-acaf-b0d9255cef1a",
   "metadata": {},
   "source": [
    "### Prueba RAPIDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a46afe21-e02e-464a-be60-6c43e83ff031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.3.247\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "print(ultralytics.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6203dc8f-f663-4bf2-bbaa-7fcafc69ec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.7 environment at: /home/alumno/py313ml/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m60 packages\u001b[0m \u001b[2min 675ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m28 packages\u001b[0m \u001b[2min 2.21s\u001b[0m\u001b[0m                                            \n",
      "\u001b[2mUninstalled \u001b[1m27 packages\u001b[0m \u001b[2min 778ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m28 packages\u001b[0m \u001b[2min 625ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1malbumentations\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1malbumentations\u001b[0m\u001b[2m==2.0.8\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.8.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2026.1.4\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.4\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.19.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.60.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.61.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.12.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.11\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.10.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.10.8\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.6.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvshmem-cu12\u001b[0m\u001b[2m==3.3.20\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mopencv-python-headless\u001b[0m\u001b[2m==4.10.0.84\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopencv-python-headless\u001b[0m\u001b[2m==4.11.0.86\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.1.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.11.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.33.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.41.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpyparsing\u001b[0m\u001b[2m==3.2.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyparsing\u001b[0m\u001b[2m==3.3.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.16.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.16.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1msimsimd\u001b[0m\u001b[2m==6.5.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msimsimd\u001b[0m\u001b[2m==6.5.12\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mstringzilla\u001b[0m\u001b[2m==4.0.14\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstringzilla\u001b[0m\u001b[2m==4.6.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.23.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.5.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1multralytics\u001b[0m\u001b[2m==8.3.238\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1multralytics\u001b[0m\u001b[2m==8.3.247\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.6.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !uv pip install -U ultralytics albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c594040-9cb0-41d4-92e6-ac9972e91534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 1 con distorsiones geom√©tricas...\n",
      "New https://pypi.org/project/ultralytics/8.4.4 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, augmentations=[GridDistortion(p=0.5, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5), ElasticTransform(p=0.5, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0), GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44))], auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=folds/fold_0.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.0, multi_scale=False, name=fold_0, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/kfold_distorted, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_0, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 426.1¬±455.1 MB/s, size: 58.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 7452 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7452/7452 2.2Kit/s 3.3s0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mGridDistortion(p=0.5, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5), ElasticTransform(p=0.5, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0), GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 213.9¬±38.8 MB/s, size: 61.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1864 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1864/1864 965.3it/s 1.9s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "Plotting labels to /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_0/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_0\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mGridDistortion(p=0.5, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5), ElasticTransform(p=0.5, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0), GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10      2.19G      1.188      1.488      1.084        122        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.7it/s 4:38<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 5.3it/s 11.2s0.2s\n",
      "                   all       1864      17172       0.79      0.699      0.783      0.602\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10      2.37G      1.084     0.9282      1.038        105        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.8it/s 4:23<0.3ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 8.3it/s 7.1s0.2s\n",
      "                   all       1864      17172      0.833       0.74      0.832      0.635\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10      2.37G       1.05     0.7986      1.025        114        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.8it/s 4:24<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.8it/s 8.6s0.1s\n",
      "                   all       1864      17172      0.849      0.794      0.871      0.676\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10      2.37G      1.013     0.7264      1.015        135        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.5it/s 5:21<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 9.2it/s 6.4s0.1ss\n",
      "                   all       1864      17172      0.889      0.833      0.902      0.711\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10      2.37G     0.9786     0.6626     0.9955        106        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.8it/s 4:13<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.8it/s 7.6s0.1s\n",
      "                   all       1864      17172      0.889       0.84      0.914      0.727\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10      2.37G     0.9425     0.6201     0.9809         92        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.8it/s 4:19<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 8.5it/s 6.9s0.1ss\n",
      "                   all       1864      17172      0.915      0.845      0.923      0.747\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10      2.38G      0.921     0.5922     0.9751         87        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:11<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 5.3it/s 11.1s0.1s\n",
      "                   all       1864      17172      0.922      0.874      0.939       0.77\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10      2.38G      0.897     0.5603     0.9653         93        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.8it/s 4:17<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 5.0it/s 11.9s0.1s\n",
      "                   all       1864      17172      0.923      0.879      0.948      0.786\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10      2.39G      0.868     0.5283     0.9545        115        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:04<0.4ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.1it/s 8.3s0.1s\n",
      "                   all       1864      17172      0.927      0.896      0.953      0.798\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10      2.39G     0.8412     0.5041     0.9461         78        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:56<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 5.7it/s 10.3s0.1s\n",
      "                   all       1864      17172      0.935      0.903      0.959      0.809\n",
      "\n",
      "10 epochs completed in 0.756 hours.\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_0/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_0/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_0/weights/best.pt...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 8.2it/s 7.2s0.1ss\n",
      "                   all       1864      17172      0.935      0.904      0.959      0.809\n",
      "                   bus        270        282      0.966      0.896      0.952      0.826\n",
      "                   car       1841      14104      0.932      0.919      0.973      0.775\n",
      "                 truck        806       1331      0.902      0.828      0.921      0.764\n",
      "                   van        765       1455      0.941      0.973      0.989      0.871\n",
      "Speed: 0.1ms preprocess, 0.3ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_0\u001b[0m\n",
      "-> Fold 0: Mejor √©poca encontrada (Ep 10) con mAP@50: 0.9586\n",
      "\n",
      "Training Fold 2 con distorsiones geom√©tricas...\n",
      "New https://pypi.org/project/ultralytics/8.4.4 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, augmentations=[GridDistortion(p=0.5, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5), ElasticTransform(p=0.5, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0), GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44))], auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=folds/fold_1.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.0, multi_scale=False, name=fold_1, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/kfold_distorted, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_1, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 848.2¬±658.2 MB/s, size: 55.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 7453 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7453/7453 2.2Kit/s 3.4s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mGridDistortion(p=0.5, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5), ElasticTransform(p=0.5, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0), GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.1 ms, read: 437.0¬±441.2 MB/s, size: 61.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 1.3Kit/s 1.5s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "Plotting labels to /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_1/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_1\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mGridDistortion(p=0.5, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5), ElasticTransform(p=0.5, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0), GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10      2.23G       1.19      1.492      1.083        122        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:04<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.3it/s 9.4s0.1s\n",
      "                   all       1863      16920       0.75      0.688      0.757      0.583\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10       2.8G      1.087     0.9266      1.039        105        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 3:60<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.9it/s 7.4s0.1ss\n",
      "                   all       1863      16920      0.762      0.752      0.822       0.63\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10       2.8G      1.043     0.7985      1.025        104        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:03<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.1it/s 8.3s0.1s\n",
      "                   all       1863      16920      0.841       0.77      0.844       0.65\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10       2.8G      1.015     0.7245      1.011        163        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:02<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 5.7it/s 10.4s0.1s\n",
      "                   all       1863      16920      0.845      0.817      0.889      0.693\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10       2.8G     0.9693     0.6635     0.9952        121        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:56<0.3ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.0it/s 8.5s0.2ss\n",
      "                   all       1863      16920      0.871      0.849      0.911      0.723\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10       2.8G     0.9413     0.6191     0.9837        120        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:04<0.3ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.8it/s 8.7s<0.2s\n",
      "                   all       1863      16920      0.879      0.859      0.925      0.752\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10       2.8G     0.9143     0.5843     0.9704        104        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:58<0.3ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.3it/s 8.0s0.2s\n",
      "                   all       1863      16920      0.892      0.853      0.929      0.758\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10       2.8G     0.8915     0.5575      0.963         86        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:02<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.2it/s 8.2s<0.2s\n",
      "                   all       1863      16920      0.888      0.895       0.94      0.778\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10       2.8G     0.8618     0.5264      0.952        106        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:51<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.6it/s 8.9s0.1s\n",
      "                   all       1863      16920      0.913      0.886      0.946      0.783\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10       2.8G     0.8457     0.5054     0.9449        110        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 3:59<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.3it/s 8.0s0.1ss\n",
      "                   all       1863      16920      0.924      0.904      0.956      0.798\n",
      "\n",
      "10 epochs completed in 0.692 hours.\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_1/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_1/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_1/weights/best.pt...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.9it/s 7.5s0.1ss\n",
      "                   all       1863      16920      0.923      0.903      0.956      0.798\n",
      "                   bus        270        288      0.941      0.889       0.95      0.802\n",
      "                   car       1830      13912      0.928      0.924      0.973       0.78\n",
      "                 truck        845       1349      0.876      0.824      0.912      0.744\n",
      "                   van        737       1371      0.949      0.977      0.989      0.864\n",
      "Speed: 0.1ms preprocess, 0.5ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_1\u001b[0m\n",
      "-> Fold 1: Mejor √©poca encontrada (Ep 10) con mAP@50: 0.9561\n",
      "\n",
      "Training Fold 3 con distorsiones geom√©tricas...\n",
      "New https://pypi.org/project/ultralytics/8.4.4 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, augmentations=[GridDistortion(p=0.5, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5), ElasticTransform(p=0.5, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0), GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44))], auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=folds/fold_2.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.0, multi_scale=False, name=fold_2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/kfold_distorted, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 606.6¬±530.9 MB/s, size: 54.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 7453 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7453/7453 1.1Kit/s 7.1ss<0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mGridDistortion(p=0.5, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5), ElasticTransform(p=0.5, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0), GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 231.5¬±214.4 MB/s, size: 61.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 784.8it/s 2.4s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "Plotting labels to /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mGridDistortion(p=0.5, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5), ElasticTransform(p=0.5, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0), GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10      2.18G      1.195      1.498      1.085        136        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:04<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 5.4it/s 10.9s0.1s\n",
      "                   all       1863      17283      0.746      0.709      0.762       0.58\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10      2.58G      1.087     0.9347      1.043         95        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:56<0.3ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 5.8it/s 10.1s0.1s\n",
      "                   all       1863      17283      0.766       0.76      0.802      0.606\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10      2.58G       1.05     0.8058      1.028        108        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:03<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.4it/s 7.9s0.1s\n",
      "                   all       1863      17283       0.85      0.787      0.844       0.66\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10      2.58G      1.015     0.7322      1.015        171        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:03<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.4it/s 9.2s0.1ss\n",
      "                   all       1863      17283      0.878      0.822      0.894      0.709\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10      2.58G     0.9759     0.6685     0.9985        132        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:56<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.8it/s 8.7s0.1s\n",
      "                   all       1863      17283      0.881       0.85      0.912      0.734\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10      2.58G     0.9449     0.6215     0.9855        115        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:02<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.5it/s 9.1s0.1s\n",
      "                   all       1863      17283      0.906       0.85      0.924      0.759\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10       2.6G     0.9203     0.5937     0.9753        113        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:01<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 8.6it/s 6.9s<0.1s\n",
      "                   all       1863      17283      0.901      0.865      0.922      0.759\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10       2.6G     0.8963     0.5579     0.9669         80        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:01<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.3it/s 9.4s0.1s\n",
      "                   all       1863      17283      0.908      0.886      0.942      0.786\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10       2.6G     0.8715     0.5318      0.956        141        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:50<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 5.7it/s 10.3s0.1s\n",
      "                   all       1863      17283      0.914      0.884      0.944      0.794\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10       2.6G     0.8528     0.5087     0.9481        104        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:56<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 5.5it/s 10.8s0.1s\n",
      "                   all       1863      17283      0.922      0.894      0.953      0.805\n",
      "\n",
      "10 epochs completed in 0.692 hours.\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_2/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_2/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_2/weights/best.pt...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 8.2it/s 7.2s<0.1s\n",
      "                   all       1863      17283      0.921      0.894      0.953      0.805\n",
      "                   bus        270        289      0.915      0.869      0.943      0.819\n",
      "                   car       1843      14182      0.941      0.911      0.971       0.78\n",
      "                 truck        820       1291      0.872      0.827       0.91      0.745\n",
      "                   van        779       1521      0.957      0.968      0.989      0.877\n",
      "Speed: 0.1ms preprocess, 0.3ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_2\u001b[0m\n",
      "-> Fold 2: Mejor √©poca encontrada (Ep 10) con mAP@50: 0.9531\n",
      "\n",
      "Training Fold 4 con distorsiones geom√©tricas...\n",
      "New https://pypi.org/project/ultralytics/8.4.4 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, augmentations=[GridDistortion(p=0.5, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5), ElasticTransform(p=0.5, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0), GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44))], auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=folds/fold_3.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.0, multi_scale=False, name=fold_3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/kfold_distorted, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 772.5¬±465.2 MB/s, size: 55.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 7453 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7453/7453 2.2Kit/s 3.3s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mGridDistortion(p=0.5, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5), ElasticTransform(p=0.5, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0), GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 431.6¬±419.2 MB/s, size: 62.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 1.3Kit/s 1.4s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "Plotting labels to /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_3\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mGridDistortion(p=0.5, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5), ElasticTransform(p=0.5, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0), GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10      2.22G      1.196      1.496      1.084        109        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:04<0.4ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.1it/s 9.7s0.1s\n",
      "                   all       1863      17211      0.766      0.692      0.761       0.58\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10      2.79G      1.084     0.9292      1.039        118        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:57<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.0it/s 9.8s0.1s\n",
      "                   all       1863      17211      0.774      0.734      0.792      0.607\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10      2.79G      1.049     0.8039      1.025        109        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:02<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.9it/s 7.4s<0.1s\n",
      "                   all       1863      17211      0.846      0.784      0.864       0.67\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10      2.79G      1.012     0.7267      1.012        148        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:05<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.0it/s 8.5s<0.1s\n",
      "                   all       1863      17211       0.87      0.829      0.892      0.708\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10      2.79G     0.9786     0.6718     0.9979        127        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:58<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.6it/s 8.9s0.2s\n",
      "                   all       1863      17211      0.876      0.855      0.915      0.726\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10      2.79G     0.9478      0.624     0.9854        110        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:02<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 5.2it/s 11.4s0.2s\n",
      "                   all       1863      17211      0.903      0.856      0.922      0.749\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10      2.79G     0.9219     0.5874     0.9728         93        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:00<0.5ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.8it/s 8.6s0.1s\n",
      "                   all       1863      17211      0.913      0.869      0.937      0.755\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10      2.79G      0.894      0.557     0.9638         88        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:03<0.3ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.1it/s 9.7s0.1s\n",
      "                   all       1863      17211      0.917      0.883      0.943      0.775\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10      2.79G     0.8687     0.5302     0.9527        113        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:53<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.1it/s 9.6s0.1ss\n",
      "                   all       1863      17211      0.928      0.886      0.949      0.786\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10      2.79G     0.8471     0.5066     0.9446        108        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:58<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.1it/s 9.7s0.2s\n",
      "                   all       1863      17211      0.928      0.904      0.956      0.801\n",
      "\n",
      "10 epochs completed in 0.696 hours.\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_3/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_3/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_3/weights/best.pt...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 8.2it/s 7.2s<0.2s\n",
      "                   all       1863      17211      0.928      0.904      0.956      0.801\n",
      "                   bus        270        292      0.947      0.908      0.959      0.818\n",
      "                   car       1837      14153      0.927      0.925      0.973       0.78\n",
      "                 truck        831       1342      0.889      0.822      0.908      0.743\n",
      "                   van        741       1424      0.947      0.962      0.984      0.862\n",
      "Speed: 0.1ms preprocess, 0.3ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_3\u001b[0m\n",
      "-> Fold 3: Mejor √©poca encontrada (Ep 10) con mAP@50: 0.9559\n",
      "\n",
      "Training Fold 5 con distorsiones geom√©tricas...\n",
      "New https://pypi.org/project/ultralytics/8.4.4 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, augmentations=[GridDistortion(p=0.5, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5), ElasticTransform(p=0.5, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0), GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44))], auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=folds/fold_4.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.0, multi_scale=False, name=fold_4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/kfold_distorted, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 445.8¬±535.4 MB/s, size: 56.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 7453 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7453/7453 2.3Kit/s 3.3s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mGridDistortion(p=0.5, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5), ElasticTransform(p=0.5, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0), GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.8¬±1.5 ms, read: 177.2¬±51.2 MB/s, size: 55.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 1.2Kit/s 1.5s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "Plotting labels to /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_4\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mGridDistortion(p=0.5, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5), ElasticTransform(p=0.5, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0), GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10      2.17G      1.197      1.501      1.087        115        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:05<0.5ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 5.7it/s 10.4s0.1s\n",
      "                   all       1863      17157      0.755      0.698      0.757       0.58\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10      2.75G      1.088     0.9365      1.041        110        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:57<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 5.9it/s 10.0s0.1s\n",
      "                   all       1863      17157      0.784      0.771      0.825      0.637\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10      2.75G      1.053     0.8069      1.028        108        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:04<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 5.9it/s 9.9s0.1ss\n",
      "                   all       1863      17157      0.848       0.77      0.841      0.647\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10      2.76G      1.012     0.7238      1.012        169        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:07<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.4it/s 9.2s0.1s\n",
      "                   all       1863      17157      0.865       0.83      0.891      0.707\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10      2.76G     0.9765     0.6693     0.9978        139        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:59<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.2it/s 9.6s0.1ss\n",
      "                   all       1863      17157      0.878      0.859      0.913      0.738\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10      2.76G     0.9403     0.6228     0.9826        121        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:07<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.5it/s 7.9s<0.2s\n",
      "                   all       1863      17157      0.902      0.846      0.918      0.739\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10      2.76G     0.9223     0.5914     0.9739        111        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:03<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.0it/s 8.5s<0.1s\n",
      "                   all       1863      17157      0.908      0.871      0.931      0.766\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10      2.76G     0.8925     0.5576      0.964        103        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 1.9it/s 4:05<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 7.3it/s 8.1s0.1s\n",
      "                   all       1863      17157      0.917      0.883      0.942      0.779\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10      2.76G     0.8646     0.5287      0.952        129        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:52<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 6.3it/s 9.4s0.1s\n",
      "                   all       1863      17157      0.922      0.882      0.945       0.79\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10      2.76G     0.8466     0.5047     0.9451        110        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 466/466 2.0it/s 3:58<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 5.5it/s 10.7s0.1s\n",
      "                   all       1863      17157      0.933      0.892      0.956      0.805\n",
      "\n",
      "10 epochs completed in 0.700 hours.\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_4/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_4/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_4/weights/best.pt...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59/59 8.0it/s 7.3s<0.2s\n",
      "                   all       1863      17157      0.933      0.892      0.956      0.805\n",
      "                   bus        270        287      0.928      0.868      0.938      0.809\n",
      "                   car       1848      14142      0.941      0.907      0.972      0.781\n",
      "                 truck        835       1356      0.896      0.828      0.925      0.765\n",
      "                   van        751       1372      0.967      0.966      0.987      0.865\n",
      "Speed: 0.1ms preprocess, 0.3ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_4\u001b[0m\n",
      "-> Fold 4: Mejor √©poca encontrada (Ep 10) con mAP@50: 0.9557\n",
      "\n",
      "==================================================\n",
      "          Precision    Recall    mAP@50  mAP@50-95\n",
      "Fold                                              \n",
      "0          0.935290  0.903380  0.958570   0.809300\n",
      "1          0.923730  0.903620  0.956130   0.797630\n",
      "2          0.921580  0.893560  0.953070   0.804740\n",
      "3          0.927780  0.904070  0.955910   0.800750\n",
      "4          0.933480  0.892140  0.955700   0.804860\n",
      "PROMEDIO   0.928372  0.899354  0.955876   0.803456\n",
      "==================================================\n",
      "Resultados guardados exitosamente en 'kfold_final_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 1. Definimos las transformaciones\n",
    "custom_transforms = [\n",
    "    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "    A.ElasticTransform(alpha=1, sigma=50, p=0.5),\n",
    "    A.GaussNoise(p=0.3),\n",
    "]\n",
    "\n",
    "# Configuraci√≥n\n",
    "num_folds = 5  \n",
    "epochs = 10\n",
    "img_size = 640\n",
    "batch_size = 16\n",
    "model_name = \"yolov8n.pt\"\n",
    "project_name = 'runs/kfold_distorted'\n",
    "\n",
    "# <--- A√ëADIDO: Faltaba inicializar la lista para guardar resultados\n",
    "results_summary = [] \n",
    "\n",
    "# 2. Bucle de entrenamiento\n",
    "for k in range(num_folds):\n",
    "    yaml_file = f'folds/fold_{k}.yaml'\n",
    "    if not os.path.exists(yaml_file): \n",
    "        continue\n",
    "\n",
    "    print(f\"\\nTraining Fold {k+1} con distorsiones geom√©tricas...\")\n",
    "    \n",
    "    # <--- A√ëADIDO: Definimos run_name aqu√≠ para usarlo en el train y en el path\n",
    "    run_name = f'fold_{k}' \n",
    "\n",
    "    # Cargar modelo\n",
    "    model = YOLO(model_name)\n",
    "    \n",
    "    model.train(\n",
    "        data=yaml_file,\n",
    "        epochs=epochs,\n",
    "        imgsz=img_size,\n",
    "        batch=batch_size,\n",
    "        project=project_name,\n",
    "        name=run_name,        # <--- AHORA USAMOS LA VARIABLE\n",
    "        patience=10, \n",
    "        verbose=True,\n",
    "        exist_ok=True,\n",
    "        augmentations=custom_transforms, \n",
    "        seed=42,\n",
    "        deterministic=True,\n",
    "        workers=4,\n",
    "        degrees=0.0,\n",
    "        mosaic=0.0,     \n",
    "        mixup=0.0,       \n",
    "        copy_paste=0.0   \n",
    "    )\n",
    "\n",
    "    # Construir ruta al CSV\n",
    "    # Ahora 'run_name' ya existe, as√≠ que esto funcionar√°\n",
    "    results_path = os.path.join(project_name, run_name, 'results.csv')\n",
    "    \n",
    "    if os.path.exists(results_path):\n",
    "        # Leemos el CSV generado por YOLO\n",
    "        df = pd.read_csv(results_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # Buscamos la mejor fila\n",
    "        best_idx = df['metrics/mAP50(B)'].idxmax()\n",
    "        best_epoch_data = df.iloc[best_idx]\n",
    "        \n",
    "        # Guardamos\n",
    "        results_summary.append({\n",
    "            'Fold': k,\n",
    "            'Precision': best_epoch_data['metrics/precision(B)'],\n",
    "            'Recall': best_epoch_data['metrics/recall(B)'],\n",
    "            'mAP@50': best_epoch_data['metrics/mAP50(B)'],\n",
    "            'mAP@50-95': best_epoch_data['metrics/mAP50-95(B)']\n",
    "        })\n",
    "        \n",
    "        print(f\"-> Fold {k}: Mejor √©poca encontrada (Ep {int(best_epoch_data['epoch'])}) con mAP@50: {best_epoch_data['metrics/mAP50(B)']:.4f}\")\n",
    "    else:\n",
    "        print(f\"Error: No se encontr√≥ el archivo de resultados en {results_path}\")\n",
    "\n",
    "# --- 3. PROCESAMIENTO FINAL Y GUARDADO ---\n",
    "\n",
    "if results_summary: # Verificaci√≥n extra por si la lista est√° vac√≠a\n",
    "    df_final = pd.DataFrame(results_summary)\n",
    "    df_final.set_index('Fold', inplace=True)\n",
    "\n",
    "    # Calculamos el promedio\n",
    "    df_final.loc['PROMEDIO'] = df_final.mean()\n",
    "\n",
    "    # Mostramos en consola\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(df_final)\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Guardamos a CSV\n",
    "    output_csv_name = 'kfold_final_results.csv'\n",
    "    df_final.to_csv(output_csv_name)\n",
    "    print(f\"Resultados guardados exitosamente en '{output_csv_name}'\")\n",
    "else:\n",
    "    print(\"No se generaron resultados para guardar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e525dbe4-9b39-4c19-8278-dbe0e700280c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GENERANDO MATRIZ ACUMULADA DE 5 FOLDS ---\n",
      "Leyendo modelos desde: runs/kfold_distorted\n",
      "\n",
      ">>> Procesando Fold 0...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.0 ms, read: 495.0¬±477.8 MB/s, size: 57.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1864 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1864/1864 2.6Kit/s 0.7s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 15.3it/s 7.6s0.1s\n",
      "                   all       1864      17172      0.935      0.903      0.959      0.809\n",
      "Speed: 0.6ms preprocess, 0.5ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val75\u001b[0m\n",
      "\n",
      ">>> Procesando Fold 1...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 304.5¬±239.8 MB/s, size: 57.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 2.1Kit/s 0.9s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 15.4it/s 7.6s0.1s\n",
      "                   all       1863      16920      0.924      0.902      0.956      0.798\n",
      "Speed: 0.6ms preprocess, 0.4ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val76\u001b[0m\n",
      "\n",
      ">>> Procesando Fold 2...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 187.9¬±29.4 MB/s, size: 52.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 2.0Kit/s 0.9s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 15.4it/s 7.6s0.2s\n",
      "                   all       1863      17283      0.922      0.893      0.953      0.805\n",
      "Speed: 0.6ms preprocess, 0.5ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val77\u001b[0m\n",
      "\n",
      ">>> Procesando Fold 3...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 504.3¬±507.8 MB/s, size: 53.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 1.8Kit/s 1.0s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 15.2it/s 7.7s0.1s\n",
      "                   all       1863      17211      0.927      0.905      0.956      0.801\n",
      "Speed: 0.6ms preprocess, 0.5ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val78\u001b[0m\n",
      "\n",
      ">>> Procesando Fold 4...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 867.4¬±225.0 MB/s, size: 53.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels... 1863 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1863/1863 2.5Kit/s 0.7s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117/117 15.2it/s 7.7s0.1s\n",
      "                   all       1863      17157      0.933      0.892      0.956      0.805\n",
      "Speed: 0.6ms preprocess, 0.4ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val79\u001b[0m\n",
      "\n",
      "Generando Gr√°fico Global...\n",
      "‚úÖ Matriz guardada en: ./normalized_matrix_kfold_final.png\n",
      "\n",
      "--- Conteos Absolutos Totales ---\n",
      "[[       1273           2          10          13         200]\n",
      " [         30       67219         944          17       10968]\n",
      " [         32         517        5511          26        1730]\n",
      " [         53          34          16        6979         717]\n",
      " [         50        2721         188         108           0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2098/3395107394.py:113: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- CONFIGURACI√ìN (Debe coincidir con tu entrenamiento anterior) ---\n",
    "num_folds = 5\n",
    "project_name = 'runs/kfold_distorted'  # <--- Ruta donde se guardaron los entrenamientos\n",
    "class_names = {0: 'bus', 1: 'car', 2: 'truck', 3: 'van'} # Ajusta esto a tus clases reales\n",
    "\n",
    "# Configuraci√≥n del gr√°fico\n",
    "formato_columnas = True # True = Transponer matriz (Columnas suman 1)\n",
    "save_matrix = True\n",
    "path_matrix = \"./normalized_matrix_kfold_final.png\"\n",
    "\n",
    "# Variable para acumular\n",
    "total_matrix = None \n",
    "\n",
    "print(f\"--- GENERANDO MATRIZ ACUMULADA DE {num_folds} FOLDS ---\")\n",
    "print(f\"Leyendo modelos desde: {project_name}\")\n",
    "\n",
    "# --- BUCLE DE LECTURA Y ACUMULACI√ìN ---\n",
    "for k in range(num_folds):\n",
    "    print(f\"\\n>>> Procesando Fold {k}...\")\n",
    "    \n",
    "    # 1. Construimos la ruta al mejor peso de ese fold\n",
    "    weights_path = os.path.join(project_name, f'fold_{k}', 'weights', 'best.pt')\n",
    "    yaml_file = f'folds/fold_{k}.yaml' # Necesitamos el YAML para validar\n",
    "    \n",
    "    # Verificamos que existan\n",
    "    if not os.path.exists(weights_path):\n",
    "        print(f\"‚ùå ERROR: No se encuentra el modelo: {weights_path}\")\n",
    "        continue \n",
    "        \n",
    "    if not os.path.exists(yaml_file):\n",
    "        print(f\"‚ùå ERROR: No se encuentra el YAML: {yaml_file}\")\n",
    "        continue\n",
    "\n",
    "    # 2. Cargamos el modelo ya entrenado\n",
    "    model = YOLO(weights_path)\n",
    "    \n",
    "    # 3. Validamos (plots=True es necesario para que calcule la matriz interna)\n",
    "    # verbose=False para que no llene la pantalla de texto\n",
    "    metrics = model.val(data=yaml_file, plots=True, verbose=False)\n",
    "    \n",
    "    # 4. Extraemos la matriz de confusi√≥n\n",
    "    # La matriz viene como objeto, accedemos a sus datos num√©ricos con .matrix\n",
    "    current_matrix = metrics.confusion_matrix.matrix\n",
    "    \n",
    "    # 5. Acumulamos (Sumamos la matriz actual a la total)\n",
    "    if total_matrix is None:\n",
    "        total_matrix = current_matrix\n",
    "    else:\n",
    "        # Verificamos dimensiones por seguridad\n",
    "        if total_matrix.shape == current_matrix.shape:\n",
    "            total_matrix += current_matrix\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Error de dimensiones en Fold {k}. Se omite.\")\n",
    "\n",
    "# --- VISUALIZACI√ìN Y GUARDADO ---\n",
    "if total_matrix is not None and total_matrix.sum() > 0:\n",
    "    print(\"\\nGenerando Gr√°fico Global...\")\n",
    "    \n",
    "    # Definir etiquetas (Clases + Background que a√±ade YOLO)\n",
    "    labels = list(class_names.values())\n",
    "    \n",
    "    # Ajuste de etiquetas si YOLO detect√≥ 'background' o menos clases\n",
    "    if total_matrix.shape[0] > len(labels):\n",
    "        labels.append('background')\n",
    "    elif total_matrix.shape[0] < len(labels):\n",
    "        # Si el modelo nunca predijo una clase, la matriz es m√°s peque√±a\n",
    "        labels = labels[:total_matrix.shape[0]]\n",
    "\n",
    "    # 1. Normalizaci√≥n por filas (Recall)\n",
    "    row_sums = total_matrix.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1 # Evitar divisi√≥n por cero\n",
    "    norm_matrix = total_matrix / row_sums\n",
    "\n",
    "    # 2. Configuraci√≥n del plot seg√∫n tu preferencia\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    if formato_columnas:\n",
    "        # Transponemos para que visualmente las columnas sumen 1\n",
    "        data_to_plot = norm_matrix.T\n",
    "        x_label = 'PREDICTIONS'\n",
    "        y_label = 'TRUTH VALUES'\n",
    "        title_suffix = \"(Transpuesta - Columnas suman 1)\"\n",
    "    else:\n",
    "        # Normal (Filas suman 1)\n",
    "        data_to_plot = norm_matrix\n",
    "        x_label = 'TRUTH VALUES' # YOLO pone Truth en X por defecto en su matriz interna\n",
    "        y_label = 'PREDICTIONS'\n",
    "        title_suffix = \"(Filas suman 1)\"\n",
    "\n",
    "    # 3. Dibujar Heatmap\n",
    "    sns.heatmap(data_to_plot, \n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap='Blues',\n",
    "                xticklabels=labels, \n",
    "                yticklabels=labels)\n",
    "    \n",
    "    plt.title(f'Matriz de Confusi√≥n Global (K-Fold Acumulado)\\n{title_suffix}\\nTotal Muestras: {int(total_matrix.sum())}')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    \n",
    "    # 4. Guardar\n",
    "    if save_matrix:\n",
    "        plt.savefig(path_matrix, dpi=300)\n",
    "        print(f\"‚úÖ Matriz guardada en: {path_matrix}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- Conteos Absolutos Totales ---\")\n",
    "    print(total_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå No se pudo generar la matriz. Revisa las rutas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d26af6-5b88-416a-97d4-bd0678950014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4378dcd3-1bd8-4a9a-9bfa-06b5f472d353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(trainer=None, **kwargs: 'Any')\n"
     ]
    }
   ],
   "source": [
    "# from ultralytics import YOLO\n",
    "# import inspect\n",
    "\n",
    "# # Cargar modelo\n",
    "# model = YOLO('yolov8n.pt')\n",
    "\n",
    "# # Inspeccionar los argumentos aceptados por el m√©todo train\n",
    "# # Nota: A veces train es un wrapper y los argumentos son **kwargs, \n",
    "# # pero esto te ayudar√° a ver la firma.\n",
    "# print(inspect.signature(model.train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd40c36e-afec-42f3-a4ac-769c28636d4b",
   "metadata": {},
   "source": [
    "## FIN PRUEBA RAPIDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80c1c76-5cb6-4759-b44f-a70b0d06de3d",
   "metadata": {},
   "source": [
    "## Ver peores imagenes distorsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30bfcaa6-8658-4b49-b2fc-ff019aab3f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Los resultados se guardar√°n en la carpeta: prueba3/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando Fold 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 87.13it/s]\n",
      "Procesando Fold 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 84.43it/s]\n",
      "Procesando Fold 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 80.73it/s]\n",
      "Procesando Fold 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 80.41it/s]\n",
      "Procesando Fold 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 83.64it/s]\n",
      "/tmp/ipykernel_2098/1838564598.py:183: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando las 10 peores im√°genes en 'prueba3'...\n",
      "\n",
      "¬°Listo! Im√°genes guardadas en la carpeta: prueba3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIGURACI√ìN ---\n",
    "folds_dir = './folds'             # Donde est√°n los txt (val_fold_0.txt...)\n",
    "project_name = 'runs/kfold_distorted' \n",
    "base_output_name = 'prueba'       # Nombre base para la carpeta (crear√° prueba1, prueba2...)\n",
    "num_folds = 5\n",
    "iou_threshold = 0.45 \n",
    "top_n_worst = 10                  # Cu√°ntas im√°genes guardar\n",
    "\n",
    "# --- CLASES ---\n",
    "CLASS_NAMES = {\n",
    "    0: 'bus',\n",
    "    1: 'car',\n",
    "    2: 'truck',\n",
    "    3: 'van'\n",
    "}\n",
    "\n",
    "# --- 1. L√ìGICA DE CARPETAS INCREMENTALES (CHECKPOINTS) ---\n",
    "def get_next_folder(base_name):\n",
    "    \"\"\"\n",
    "    Busca la siguiente carpeta libre: prueba1, prueba2, prueba3...\n",
    "    \"\"\"\n",
    "    i = 1\n",
    "    while True:\n",
    "        folder_name = f\"{base_name}{i}\"\n",
    "        if not os.path.exists(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "            return folder_name\n",
    "        i += 1\n",
    "\n",
    "# Creamos la nueva carpeta autom√°ticamente\n",
    "save_dir = get_next_folder(base_output_name)\n",
    "print(f\"--> Los resultados se guardar√°n en la carpeta: {save_dir}/\")\n",
    "\n",
    "# --- FUNCIONES AUXILIARES ---\n",
    "def get_class_name(cls_id):\n",
    "    return CLASS_NAMES.get(int(cls_id), str(int(cls_id)))\n",
    "\n",
    "def calculate_iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    return interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "\n",
    "def analyze_image(model, img_path, fold_idx):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None: return None\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    # 1. Ground Truth\n",
    "    label_path = img_path.replace('/images/', '/labels/').replace('.jpg', '.txt').replace('.png', '.txt')\n",
    "    gt_boxes = []\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = list(map(float, line.strip().split()))\n",
    "                if len(parts) >= 5:\n",
    "                    c, x, y, bw, bh = parts[:5]\n",
    "                    l, t, r, b = int((x - bw/2) * w), int((y - bh/2) * h), int((x + bw/2) * w), int((y + bh/2) * h)\n",
    "                    gt_boxes.append([l, t, r, b, int(c)])\n",
    "\n",
    "    # 2. Predicciones\n",
    "    results = model(img, conf=0.25, verbose=False)\n",
    "    pred_boxes = []\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            b_c = box.xyxy[0].cpu().numpy().astype(int)\n",
    "            pred_boxes.append([b_c[0], b_c[1], b_c[2], b_c[3], int(box.cls), float(box.conf)])\n",
    "\n",
    "    # 3. Calcular Error\n",
    "    missed = 0\n",
    "    gt_matched = [False] * len(gt_boxes)\n",
    "    pred_matched = [False] * len(pred_boxes)\n",
    "\n",
    "    for i, gt in enumerate(gt_boxes):\n",
    "        best_iou = 0\n",
    "        best_pred_idx = -1\n",
    "        for j, pred in enumerate(pred_boxes):\n",
    "            if pred_matched[j]: continue\n",
    "            iou = calculate_iou(gt[:4], pred[:4])\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_pred_idx = j\n",
    "        \n",
    "        if best_iou >= iou_threshold:\n",
    "            gt_matched[i] = True\n",
    "            pred_matched[best_pred_idx] = True\n",
    "        else:\n",
    "            missed += 1\n",
    "\n",
    "    false_positives = pred_matched.count(False)\n",
    "    error_score = missed + false_positives\n",
    "\n",
    "    return {\n",
    "        'path': img_path,\n",
    "        'fold': fold_idx,\n",
    "        'score': error_score,\n",
    "        'gt': gt_boxes,\n",
    "        'pred': pred_boxes,\n",
    "        'img': img \n",
    "    }\n",
    "\n",
    "# --- PROCESO PRINCIPAL ---\n",
    "analyzed_images = []\n",
    "\n",
    "for k in range(num_folds):\n",
    "    txt_file = os.path.join(folds_dir, f'val_fold_{k}.txt')\n",
    "    if not os.path.exists(txt_file): \n",
    "        print(f\"No existe {txt_file}, saltando...\")\n",
    "        continue\n",
    "\n",
    "    with open(txt_file, 'r') as f:\n",
    "        img_paths = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # IMPORTANTE: Ajusta este slice si quieres procesar m√°s o menos im√°genes\n",
    "    paths_to_check = img_paths[:50] \n",
    "\n",
    "    model_path = os.path.join(project_name, f'fold_{k}', 'weights', 'best.pt')\n",
    "    if not os.path.exists(model_path): \n",
    "        print(f\"Modelo fold {k} no encontrado.\")\n",
    "        continue\n",
    "    \n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    for p in tqdm(paths_to_check, desc=f\"Procesando Fold {k}\"):\n",
    "        if not os.path.exists(p): continue\n",
    "        result = analyze_image(model, p, k)\n",
    "        # Guardamos si tiene alg√∫n error (score > 0)\n",
    "        if result and result['score'] > 0: \n",
    "            analyzed_images.append(result)\n",
    "\n",
    "# --- GUARDADO Y VISUALIZACI√ìN ---\n",
    "\n",
    "# Ordenar por error (De m√°s errores a menos)\n",
    "analyzed_images.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "print(f\"\\nGuardando las {top_n_worst} peores im√°genes en '{save_dir}'...\")\n",
    "\n",
    "for i, item in enumerate(analyzed_images[:top_n_worst]):\n",
    "    img = item['img'].copy()\n",
    "    gt = item['gt']\n",
    "    pred = item['pred']\n",
    "    filename = os.path.basename(item['path'])\n",
    "    \n",
    "    # A. DIBUJAR REAL (Verde)\n",
    "    for box in gt:\n",
    "        x1, y1, x2, y2, cls = box\n",
    "        label = f\"REAL: {get_class_name(cls)}\"\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(img, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "    \n",
    "    # B. DIBUJAR PREDICCI√ìN (Rojo)\n",
    "    for box in pred:\n",
    "        x1, y1, x2, y2, cls, conf = box\n",
    "        label = f\"PRED: {get_class_name(cls)} ({conf:.2f})\"\n",
    "        y_txt = y2 + 20 \n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        cv2.putText(img, label, (x1, y_txt), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "    # --- GUARDAR EN DISCO EN LA NUEVA CARPETA ---\n",
    "    save_name = f\"rank{i+1}_fold{item['fold']}_err{item['score']}_{filename}\"\n",
    "    save_path = os.path.join(save_dir, save_name)\n",
    "    \n",
    "    cv2.imwrite(save_path, img)\n",
    "    \n",
    "    # --- MOSTRAR EN PANTALLA ---\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(f\"TOP {i+1} | Fold {item['fold']} | Errores: {item['score']}\\nGuardado en: {save_path}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n¬°Listo! Im√°genes guardadas en la carpeta: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd3742-f48d-465a-ad43-5596dd97cae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4538dc67-9dfa-4816-b4a5-7c9dbe3563a9",
   "metadata": {},
   "source": [
    "Vemos que las peores imagenes clasificadas es por encontrar muchso errores con las etiquetas reales. Sin embargo, estos errores en la gran mayor√≠a de casos no son errores reales sino que muchos de los objetos detectados realmente existen lo que pasa es que no estan etiquetados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb68b3-4d04-4e0d-b86a-58f62272493f",
   "metadata": {},
   "source": [
    "# Inicio uso SAHI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e6df9-f705-4999-91a7-b1ee4d663090",
   "metadata": {},
   "source": [
    "# Libreria SAHI Deteccion coches arcen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f209fee7-26e7-4375-8622-532ace6cb255",
   "metadata": {},
   "source": [
    "En primer lugar, hay que importar la libreria sahi. La importamos en la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b88a3a5c-e308-4fb9-baab-8f8f66bcc6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.7 environment at: /home/alumno/py313ml/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m43 packages\u001b[0m \u001b[2min 471ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m5 packages\u001b[0m \u001b[2min 274ms\u001b[0m\u001b[0m                                             \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 10ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfire\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpybboxes\u001b[0m\u001b[2m==0.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msahi\u001b[0m\u001b[2m==0.11.36\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshapely\u001b[0m\u001b[2m==2.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mterminaltables\u001b[0m\u001b[2m==3.1.10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !uv pip install sahi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd0920e5-0fce-424c-af4a-826907a5a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sahi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f9808-bcb7-470d-9fac-8da5c7dcbd71",
   "metadata": {},
   "source": [
    "Vamos a coger esta imagen como prueba para ver si conseguimos detectar los coches del arcen la imagen es 'MVI_20011_img00005_jpg.rf.d97da4c37520d99bca7a6cb0aed56873.jpg'. Lo que haremos sera mirar la foto como esta detectada sin sahi y con sahi. Para ver las diferencias. La ruta de la imagen es ./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/images/'MVI_20011_img00005_jpg.rf.d97da4c37520d99bca7a6cb0aed56873.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d96e8d0-6d98-4c17-bb78-f9a577a2f4cb",
   "metadata": {},
   "source": [
    "## Analisis de Sahi para una √∫nica imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a57c3d49-d8c2-4905-b97e-c2ef79bb2564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando YOLO normal...\n",
      "\n",
      "image 1/1 /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/images/MVI_20011_img00005_jpg.rf.d97da4c37520d99bca7a6cb0aed56873.jpg: 640x640 7 cars, 4.8ms\n",
      "Speed: 2.2ms preprocess, 4.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Ejecutando SAHI (Slice 160x160)...\n",
      "Performing prediction on 25 slices.\n",
      "Guardando resultado en: ./Fotos pruebas con sahi/Foto unica/Comparacion_No_Sahi_vs_Sahi_kfold_distorted_160_confidence_0.6_overlap_0.2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2098/616421514.py:110: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Importaciones de SAHI\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "from sahi.utils.cv import visualize_object_predictions\n",
    "\n",
    "# --- CONFIGURACI√ìN DE USUARIO ---\n",
    "# tipo_modelo = 'kfold'\n",
    "tipo_modelo = 'kfold_distorted'\n",
    "num_fold = 2\n",
    "\n",
    "# Ruta de la imagen\n",
    "img_path = \"./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/images/MVI_20011_img00005_jpg.rf.d97da4c37520d99bca7a6cb0aed56873.jpg\"\n",
    "\n",
    "# Ruta del modelo\n",
    "model_path = f\"runs/{tipo_modelo}/fold_{num_fold}/weights/best.pt\" \n",
    "\n",
    "# Configuraci√≥n de SAHI\n",
    "slice_h = 160  # Tu configuraci√≥n: 160px\n",
    "slice_w = 160  \n",
    "overlap_h = 0.2\n",
    "overlap_w = 0.2\n",
    "confidence = 0.6 \n",
    "\n",
    "# --- CREAR DIRECTORIO DE GUARDADO (IMPORTANTE) ---\n",
    "# Esto evita errores si la carpeta no existe\n",
    "save_dir = './Fotos pruebas con sahi/Foto unica/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, f'Comparacion_No_Sahi_vs_Sahi_{tipo_modelo}_{slice_h}_confidence_{confidence}_overlap_{overlap_h}.png')\n",
    "\n",
    "# --- 1. CARGAR IMAGEN ORIGINAL ---\n",
    "img_raw = cv2.imread(img_path)\n",
    "if img_raw is None:\n",
    "    raise FileNotFoundError(f\"No se encuentra la imagen: {img_path}\")\n",
    "# Convertir a RGB para matplotlib\n",
    "img_rgb_original = cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# --- 2. PREDICCI√ìN EST√ÅNDAR (SIN SAHI) ---\n",
    "print(\"Ejecutando YOLO normal...\")\n",
    "model = YOLO(model_path)\n",
    "results_std = model(img_path, conf=confidence)\n",
    "\n",
    "# Generar imagen pintada\n",
    "res_plotted_std = results_std[0].plot() # Devuelve BGR\n",
    "img_std_rgb = cv2.cvtColor(res_plotted_std, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# --- 3. PREDICCI√ìN CON SAHI ---\n",
    "print(f\"Ejecutando SAHI (Slice {slice_h}x{slice_w})...\")\n",
    "\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov8',\n",
    "    model_path=model_path,\n",
    "    confidence_threshold=confidence,\n",
    "    device='cuda:0' # Cambia a 'cpu' si necesario\n",
    ")\n",
    "\n",
    "result_sahi = get_sliced_prediction(\n",
    "    img_path,\n",
    "    detection_model,\n",
    "    slice_height=slice_h,\n",
    "    slice_width=slice_w,\n",
    "    overlap_height_ratio=overlap_h,\n",
    "    overlap_width_ratio=overlap_w\n",
    ")\n",
    "\n",
    "# Visualizar SAHI\n",
    "# Pasamos la imagen original en BGR (img_raw) porque visualize_object_predictions maneja cv2 internamente\n",
    "visualization_result = visualize_object_predictions(\n",
    "    image=img_raw, \n",
    "    object_prediction_list=result_sahi.object_prediction_list,\n",
    "    rect_th=2,\n",
    "    text_size=0.4,\n",
    "    text_th=1\n",
    ")\n",
    "# SAHI suele devolver un array RGB listo para plotear, pero por seguridad aseguramos formato numpy\n",
    "img_sahi_result = visualization_result[\"image\"] \n",
    "\n",
    "# --- 4. MOSTRAR Y GUARDAR (LAYOUT PIRAMIDAL) ---\n",
    "plt.figure(figsize=(16, 12)) # Lienzo m√°s alto para que quepa todo bien\n",
    "\n",
    "# A. IMAGEN ORIGINAL (Arriba, ocupa todo el ancho)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(img_rgb_original)\n",
    "plt.title(f\"IMAGEN ORIGINAL\\n{os.path.basename(img_path)}\", fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "\n",
    "# B. SIN SAHI (Abajo Izquierda)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(img_std_rgb)\n",
    "plt.title(f\"YOLO Est√°ndar (Sin SAHI)\\nConf: {confidence} | Detectados: {len(results_std[0].boxes)}\", fontsize=12)\n",
    "plt.axis('off')\n",
    "\n",
    "# C. CON SAHI (Abajo Derecha)\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(img_sahi_result)\n",
    "plt.title(f\"YOLO + SAHI (Ventanas {slice_h}x{slice_w})\\nConf: {confidence} | Detectados: {len(result_sahi.object_prediction_list)}\", fontsize=12)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar\n",
    "print(f\"Guardando resultado en: {save_path}\")\n",
    "plt.savefig(save_path, dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa60582-7cf8-4266-89c5-8169465fdd75",
   "metadata": {},
   "source": [
    "Los mejores resultados obtenidos para el modelo sin distorsionar es kfold_3. Adem√°s con una confianza de 0.6 y un overlap de 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3ddbab-5b45-440d-8d1c-bed34caab89f",
   "metadata": {},
   "source": [
    "Nos quedamos con el modelo de grid con confianza de 0.6 y luego de overlap 0.2 y tama√±o de 160 x 160"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e370ca4-236b-4da8-9e98-0d4f35f79d27",
   "metadata": {},
   "source": [
    "## Procesamiento de todas las imagenes con las nuevas etiquetas creadas con SAHI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b56e7-f2f8-4a77-881d-bebd4542931c",
   "metadata": {},
   "source": [
    "El siguiente codigo lo que hace es \n",
    "Pinta TODAS las cajas que ve SAHI.\n",
    "\n",
    "- El problema: Si en una foto hay 10 coches y t√∫ ya ten√≠as etiquetados 9, este c√≥digo te pintar√° 10 cajas rojas. Tendr√°s que jugar a \"las 7 diferencias\" mirando la foto para adivinar cu√°l es la nueva.\n",
    "- Utilidad: Comprobacion de que el modelo funciona bien las fotos se guardan aqui **./Procesadas_Train_SAHI**\n",
    "- El archivo py donde se ha ejecutado esto se llama Paso1_Sahi_visualizaci√≥n.py y se ha hecho una confianza de 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08825089-60da-4860-bea0-4d69992d6ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIANDO PROCESO MASIVO ---\n",
      "Entrada: ./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/images/\n",
      "Salida:  ./Procesadas_Train_SAHI/Train_Processed_SAHI_160px_conf0.5\n",
      "Cargando modelo en memoria...\n",
      "Se encontraron 9316 im√°genes para procesar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando im√°genes con SAHI: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9316/9316 [37:00<00:00,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "¬°Proceso terminado!\n",
      "Im√°genes guardadas en: ./Procesadas_Train_SAHI/Train_Processed_SAHI_160px_conf0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Este codigo es unicamente para comprobar si el modelo funciona bien \n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm # Barra de progreso\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Importaciones de SAHI\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "from sahi.utils.cv import visualize_object_predictions\n",
    "\n",
    "# --- CONFIGURACI√ìN DE USUARIO ---\n",
    "tipo_modelo = 'kfold_distorted'\n",
    "num_fold = 2\n",
    "\n",
    "# RUTAS\n",
    "input_dir = \"./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/images/\"\n",
    "model_path = f\"runs/{tipo_modelo}/fold_{num_fold}/weights/best.pt\" \n",
    "\n",
    "# Configuraci√≥n de SAHI\n",
    "slice_h = 160 \n",
    "slice_w = 160   \n",
    "overlap_h = 0.2\n",
    "overlap_w = 0.2\n",
    "confidence = 0.5 \n",
    "\n",
    "# --- PREPARACI√ìN DEL DIRECTORIO DE SALIDA ---\n",
    "# Nombre descriptivo para saber qu√© configuraci√≥n se us√≥\n",
    "output_folder_name = f\"Train_Processed_SAHI_{slice_h}px_conf{confidence}\"\n",
    "output_dir = os.path.join('./Procesadas_Train_SAHI', output_folder_name)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"--- INICIANDO PROCESO MASIVO ---\")\n",
    "print(f\"Entrada: {input_dir}\")\n",
    "print(f\"Salida:  {output_dir}\")\n",
    "\n",
    "# --- 1. CARGAR MODELO (UNA SOLA VEZ) ---\n",
    "print(\"Cargando modelo en memoria...\")\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov8',\n",
    "    model_path=model_path,\n",
    "    confidence_threshold=confidence,\n",
    "    device='cuda:0' # Cambia a 'cpu' si no tienes GPU\n",
    ")\n",
    "\n",
    "# --- 2. OBTENER LISTA DE IM√ÅGENES ---\n",
    "# Busca todas las imagenes jpg en la carpeta\n",
    "img_paths = glob.glob(os.path.join(input_dir, \"*.jpg\"))\n",
    "print(f\"Se encontraron {len(img_paths)} im√°genes para procesar.\")\n",
    "\n",
    "# Si quieres probar primero con pocas im√°genes, descomenta la siguiente l√≠nea:\n",
    "# img_paths = img_paths[:20] \n",
    "\n",
    "# --- 3. BUCLE DE PROCESAMIENTO ---\n",
    "for img_file in tqdm(img_paths, desc=\"Procesando im√°genes con SAHI\"):\n",
    "    \n",
    "    # A. Leer imagen\n",
    "    img_raw = cv2.imread(img_file)\n",
    "    if img_raw is None:\n",
    "        print(f\"Error leyendo: {img_file}\")\n",
    "        continue\n",
    "        \n",
    "    # B. Predicci√≥n con SAHI (Slicing)\n",
    "    result_sahi = get_sliced_prediction(\n",
    "        img_file, \n",
    "        detection_model,\n",
    "        slice_height=slice_h,\n",
    "        slice_width=slice_w,\n",
    "        overlap_height_ratio=overlap_h,\n",
    "        overlap_width_ratio=overlap_w,\n",
    "        verbose=0 # Silenciar logs por cada imagen\n",
    "    )\n",
    "\n",
    "    # C. Pintar las cajas en la imagen\n",
    "    # visualize_object_predictions devuelve un dict con la imagen pintada\n",
    "    visualization_result = visualize_object_predictions(\n",
    "        image=img_raw, \n",
    "        object_prediction_list=result_sahi.object_prediction_list,\n",
    "        rect_th=2,\n",
    "        text_size=0.4,\n",
    "        text_th=1\n",
    "    )\n",
    "    \n",
    "    img_result = visualization_result[\"image\"]\n",
    "\n",
    "    # D. Convertir a BGR para guardar con OpenCV\n",
    "    # SAHI/Matplotlib trabajan en RGB, OpenCV en BGR.\n",
    "    # Si la imagen sale con colores raros (azul en vez de rojo), descomenta la linea de abajo:\n",
    "    img_result_bgr = cv2.cvtColor(img_result, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # E. Guardar imagen\n",
    "    filename = os.path.basename(img_file)\n",
    "    save_path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    cv2.imwrite(save_path, img_result_bgr)\n",
    "\n",
    "print(\"\\n¬°Proceso terminado!\")\n",
    "print(f\"Im√°genes guardadas en: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb05ce9b-c408-4413-ab51-64959ac4cfc1",
   "metadata": {},
   "source": [
    "# CODIGO PARA QUE SAHI COMPARE LAS DOS IMAGENES Y NOS QUEDAMOS LAS QUE NO ESTEN DUPLICADAS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bee861-969f-4fe3-ae9e-0daa435e3763",
   "metadata": {},
   "source": [
    "En el siguiente codigo lo que vamos a hacer es de los objetos nuevos que se detectan con sahi, las comparamos con los objetos que ya estaban detectados en nuestro dataset original. Si las cajas de los objetos coinciden en al menos un 30% estas cajas se eliminan ya que estan duplicadas y no debemos a√±adirlas porque le estariamos diciendo que ah√≠ hay dos objetos cuando realmente solo hay 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f2d80-ad26-48d9-878c-a542476592fd",
   "metadata": {},
   "source": [
    "En la siguiente celda, eliminamos los cuadrados que tengan una confianza de mas de 0.75 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2804ead9-6daa-403b-85a2-0c695df9ebf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIANDO AN√ÅLISIS DE HISTOGRAMA ---\n",
      "Modelo: /home/alumno/Desktop/datos/Computer_vision/final_project/runs/kfold_distorted/fold_2/weights/best.pt\n",
      "Buscando nuevas cajas con Confianza > 0.5\n",
      "Ignorando cajas que solapen > 30.0% con el Ground Truth\n",
      "Cargando modelo...\n",
      "Analizando 9316 im√°genes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculando estad√≠sticas:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 3608/9316 [14:00<22:09,  4.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 121\u001b[39m\n\u001b[32m    118\u001b[39m gt_list = load_ground_truth(txt_path, w, h)\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# Predicci√≥n SAHI\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m result_sahi = \u001b[43mget_sliced_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdetection_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mslice_height\u001b[49m\u001b[43m=\u001b[49m\u001b[43mslice_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mslice_width\u001b[49m\u001b[43m=\u001b[49m\u001b[43mslice_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlap_height_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlap_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlap_width_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlap_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m    129\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Filtrado estad√≠stico\u001b[39;00m\n\u001b[32m    132\u001b[39m original_preds = result_sahi.object_prediction_list\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/sahi/predict.py:270\u001b[39m, in \u001b[36mget_sliced_prediction\u001b[39m\u001b[34m(image, detection_model, slice_height, slice_width, overlap_height_ratio, overlap_width_ratio, perform_standard_pred, postprocess_type, postprocess_match_metric, postprocess_match_threshold, postprocess_class_agnostic, verbose, merge_buffer_length, auto_slice_resolution, slice_export_prefix, slice_dir, exclude_classes_by_name, exclude_classes_by_id)\u001b[39m\n\u001b[32m    268\u001b[39m     shift_amount_list.append(slice_image_result.starting_pixels[group_ind * num_batch + image_ind])\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# perform batch prediction\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m prediction_result = \u001b[43mget_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_list\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdetection_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdetection_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshift_amount\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshift_amount_list\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mslice_image_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43moriginal_image_height\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mslice_image_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43moriginal_image_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexclude_classes_by_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_classes_by_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexclude_classes_by_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_classes_by_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# convert sliced predictions to full predictions\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m object_prediction \u001b[38;5;129;01min\u001b[39;00m prediction_result.object_prediction_list:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/sahi/predict.py:98\u001b[39m, in \u001b[36mget_prediction\u001b[39m\u001b[34m(image, detection_model, shift_amount, full_shape, postprocess, verbose, exclude_classes_by_name, exclude_classes_by_id)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# get prediction\u001b[39;00m\n\u001b[32m     97\u001b[39m time_start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[43mdetection_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mperform_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_as_pil\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m time_end = time.time() - time_start\n\u001b[32m    100\u001b[39m durations_in_seconds[\u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m] = time_end\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/sahi/models/ultralytics.py:85\u001b[39m, in \u001b[36mUltralyticsDetectionModel.perform_inference\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.image_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     83\u001b[39m     kwargs = {\u001b[33m\"\u001b[39m\u001b[33mimgsz\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.image_size, **kwargs}\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m prediction_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# YOLO expects numpy arrays to have BGR\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Handle different result types for PyTorch vs ONNX models\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# ONNX models might return results in a different format\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_mask:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/ultralytics/engine/model.py:177\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, source, stream, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    150\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    151\u001b[39m     source: \u001b[38;5;28mstr\u001b[39m | Path | \u001b[38;5;28mint\u001b[39m | Image.Image | \u001b[38;5;28mlist\u001b[39m | \u001b[38;5;28mtuple\u001b[39m | np.ndarray | torch.Tensor = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    152\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    153\u001b[39m     **kwargs: Any,\n\u001b[32m    154\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    155\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[32m    156\u001b[39m \n\u001b[32m    157\u001b[39m \u001b[33;03m    This method simplifies the process of making predictions by allowing the model instance to be called directly\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    175\u001b[39m \u001b[33;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/ultralytics/engine/model.py:535\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    534\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/ultralytics/engine/predictor.py:225\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:38\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     42\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/ultralytics/engine/predictor.py:330\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.embed:\n\u001b[32m    332\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/ultralytics/engine/predictor.py:182\u001b[39m, in \u001b[36mBasePredictor.inference\u001b[39m\u001b[34m(self, im, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[32m    177\u001b[39m visualize = (\n\u001b[32m    178\u001b[39m     increment_path(\u001b[38;5;28mself\u001b[39m.save_dir / Path(\u001b[38;5;28mself\u001b[39m.batch[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).stem, mkdir=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.visualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.tensor)\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    181\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/ultralytics/nn/autobackend.py:695\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed, **kwargs)\u001b[39m\n\u001b[32m    693\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/ultralytics/nn/tasks.py:137\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/ultralytics/nn/tasks.py:154\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/ultralytics/nn/tasks.py:176\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    177\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/ultralytics/nn/modules/head.py:120\u001b[39m, in \u001b[36mDetect.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.forward_end2end(x)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.nl):\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     x[i] = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv3\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:  \u001b[38;5;66;03m# Training path\u001b[39;00m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#CODIGO 3\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# Configurar backend para que funcione sin monitor (Headless)\n",
    "matplotlib.use('Agg') \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Importaciones de SAHI\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "\n",
    "# ==========================================\n",
    "#        CONFIGURACI√ìN DEL USUARIO\n",
    "# ==========================================\n",
    "\n",
    "# 1. Rutas ABSOLUTAS (Para seguridad en el servidor)\n",
    "BASE_PROJECT_DIR = \"/home/alumno/Desktop/datos/Computer_vision/final_project\"\n",
    "DATASET_DIR = os.path.join(BASE_PROJECT_DIR, \"UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8\")\n",
    "\n",
    "input_dir = os.path.join(DATASET_DIR, \"train/images\")\n",
    "labels_dir = os.path.join(DATASET_DIR, \"train/labels\")\n",
    "\n",
    "# Configuraci√≥n del modelo\n",
    "tipo_modelo = 'kfold_distorted'\n",
    "num_fold = 2\n",
    "model_path = os.path.join(BASE_PROJECT_DIR, f\"runs/{tipo_modelo}/fold_{num_fold}/weights/best.pt\")\n",
    "\n",
    "# 2. Configuraci√≥n del an√°lisis\n",
    "confidence_threshold = 0.5     # Tu umbral elegido para explorar\n",
    "iou_threshold_exclusion = 0.30 # Umbral estricto para ignorar duplicados\n",
    "\n",
    "# 3. Parametros SAHI\n",
    "slice_h, slice_w = 160, 160\n",
    "overlap_h, overlap_w = 0.2, 0.2\n",
    "\n",
    "# 4. Salida\n",
    "output_dir_stats = os.path.join(BASE_PROJECT_DIR, \"Estadisticas_SAHI\")\n",
    "os.makedirs(output_dir_stats, exist_ok=True)\n",
    "plot_filename = os.path.join(output_dir_stats, f\"Histograma_Confianza_NuevasDetecciones_IoU{iou_threshold_exclusion}.png\")\n",
    "\n",
    "# ==========================================\n",
    "#           FUNCIONES AUXILIARES\n",
    "# ==========================================\n",
    "\n",
    "def get_iou(boxA, boxB):\n",
    "    # Calcula Intersection over Union\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    \n",
    "    return interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "\n",
    "def load_ground_truth(txt_path, img_w, img_h):\n",
    "    gt_boxes = []\n",
    "    if os.path.exists(txt_path):\n",
    "        with open(txt_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = list(map(float, line.strip().split()))\n",
    "                if len(parts) >= 5:\n",
    "                    cls, cx, cy, bw, bh = parts[:5]\n",
    "                    l = int((cx - bw/2) * img_w)\n",
    "                    t = int((cy - bh/2) * img_h)\n",
    "                    r = int((cx + bw/2) * img_w)\n",
    "                    b = int((cy + bh/2) * img_h)\n",
    "                    gt_boxes.append([l, t, r, b]) # No necesitamos la clase para calcular IoU\n",
    "    return gt_boxes\n",
    "\n",
    "# ==========================================\n",
    "#           L√ìGICA PRINCIPAL\n",
    "# ==========================================\n",
    "\n",
    "print(f\"--- INICIANDO AN√ÅLISIS DE HISTOGRAMA ---\")\n",
    "print(f\"Modelo: {model_path}\")\n",
    "print(f\"Buscando nuevas cajas con Confianza > {confidence_threshold}\")\n",
    "print(f\"Ignorando cajas que solapen > {iou_threshold_exclusion*100}% con el Ground Truth\")\n",
    "\n",
    "# 1. CARGAR MODELO\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"¬°ERROR! No se encuentra el archivo .pt en: {model_path}\")\n",
    "    exit()\n",
    "\n",
    "print(\"Cargando modelo...\")\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov8',\n",
    "    model_path=model_path,\n",
    "    confidence_threshold=confidence_threshold,\n",
    "    device='cuda:0'\n",
    ")\n",
    "\n",
    "# 2. PROCESAMIENTO\n",
    "img_paths = glob.glob(os.path.join(input_dir, \"*.jpg\"))\n",
    "surviving_confidences = []\n",
    "\n",
    "print(f\"Analizando {len(img_paths)} im√°genes...\")\n",
    "\n",
    "for img_file in tqdm(img_paths, desc=\"Calculando estad√≠sticas\"):\n",
    "    # Leer imagen\n",
    "    img_raw = cv2.imread(img_file)\n",
    "    if img_raw is None: continue\n",
    "    h, w, _ = img_raw.shape\n",
    "    \n",
    "    # Rutas etiquetas\n",
    "    filename = os.path.basename(img_file)\n",
    "    txt_name = os.path.splitext(filename)[0] + \".txt\"\n",
    "    txt_path = os.path.join(labels_dir, txt_name)\n",
    "    \n",
    "    # Cargar Ground Truth\n",
    "    gt_list = load_ground_truth(txt_path, w, h)\n",
    "    \n",
    "    # Predicci√≥n SAHI\n",
    "    result_sahi = get_sliced_prediction(\n",
    "        img_file,\n",
    "        detection_model,\n",
    "        slice_height=slice_h,\n",
    "        slice_width=slice_w,\n",
    "        overlap_height_ratio=overlap_h,\n",
    "        overlap_width_ratio=overlap_w,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Filtrado estad√≠stico\n",
    "    original_preds = result_sahi.object_prediction_list\n",
    "    \n",
    "    for pred in original_preds:\n",
    "        pred_box = pred.bbox.to_xyxy()\n",
    "        conf_value = pred.score.value \n",
    "        \n",
    "        is_redundant = False\n",
    "        \n",
    "        # Comprobar overlap con GT\n",
    "        for gt_box in gt_list:\n",
    "            if get_iou(pred_box, gt_box) > iou_threshold_exclusion:\n",
    "                is_redundant = True\n",
    "                break \n",
    "        \n",
    "        # Si NO es redundante, la contamos para la gr√°fica\n",
    "        if not is_redundant:\n",
    "            surviving_confidences.append(conf_value)\n",
    "\n",
    "# 3. GENERAR HISTOGRAMA\n",
    "print(f\"\\nGenerando gr√°fica con {len(surviving_confidences)} detecciones nuevas...\")\n",
    "\n",
    "if len(surviving_confidences) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Histograma\n",
    "    plt.hist(surviving_confidences, bins=30, color='#4CAF50', edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    # L√≠neas de media\n",
    "    mean_conf = np.mean(surviving_confidences)\n",
    "    plt.axvline(mean_conf, color='red', linestyle='dashed', linewidth=1, label=f'Media: {mean_conf:.2f}')\n",
    "    \n",
    "    plt.title(f'Confianza de Nuevas Detecciones (Filtradas)\\nIoU Threshold > {iou_threshold_exclusion}', fontsize=14)\n",
    "    plt.xlabel('Confianza (0.0 - 1.0)', fontsize=12)\n",
    "    plt.ylabel('Frecuencia', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Guardar\n",
    "    plt.savefig(plot_filename, dpi=300)\n",
    "    print(f\"‚úÖ GR√ÅFICA GUARDADA EN: {plot_filename}\")\n",
    "    \n",
    "    # Print Estad√≠sticas\n",
    "    print(\"\\n--- ESTAD√çSTICAS FINALES ---\")\n",
    "    print(f\"Total nuevas cajas encontradas: {len(surviving_confidences)}\")\n",
    "    print(f\"Confianza Promedio: {mean_conf:.4f}\")\n",
    "    print(f\"M√≠nima: {np.min(surviving_confidences):.4f}\")\n",
    "    print(f\"M√°xima: {np.max(surviving_confidences):.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron cajas nuevas con estos par√°metros.\")\n",
    "\n",
    "print(\"\\nFin del programa.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b8fb5-0397-4c53-9706-2e9c3d94dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codigo 1\n",
    "#Compararci√≥n objetos imagenes originales e imagenes nuevas\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Importaciones de SAHI\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "from sahi.utils.cv import visualize_object_predictions\n",
    "\n",
    "# --- CONFIGURACI√ìN DE USUARIO ---\n",
    "tipo_modelo = 'kfold_distorted'\n",
    "num_fold = 2\n",
    "confidence = 0.5\n",
    "\n",
    "# RUTAS\n",
    "input_dir = \"./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/images/\"\n",
    "labels_dir = \"./UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels/\" \n",
    "model_path = f\"runs/{tipo_modelo}/fold_{num_fold}/weights/best.pt\" \n",
    "\n",
    "# CONFIGURACI√ìN SAHI\n",
    "slice_h, slice_w = 160, 160\n",
    "overlap_h, overlap_w = 0.2, 0.2\n",
    "\n",
    "# --- CONFIGURACI√ìN DE FILTRADO ---\n",
    "# Subimos a 0.30. Significa que las cajas deben coincidir en un 30% de su √°rea para considerarse duplicadas.\n",
    "iou_threshold_exclusion = 0.30 \n",
    "\n",
    "# Salida\n",
    "output_folder_name = f\"Train_SAHI_Only_New_Detections_IoU{iou_threshold_exclusion}\"\n",
    "output_dir = os.path.join('./Procesadas_Train_SAHI_Filtradas', output_folder_name)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"--- INICIANDO PROCESO DE DETECCI√ìN DE ETIQUETAS FALTANTES ---\")\n",
    "print(f\"Umbral IoU: {iou_threshold_exclusion}\")\n",
    "print(f\"Condici√≥n: Se eliminar√° la caja si se solapan, SIN importar la clase.\")\n",
    "\n",
    "# --- FUNCIONES ---\n",
    "\n",
    "def get_iou(boxA, boxB):\n",
    "    # box format: [x1, y1, x2, y2]\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    \n",
    "    return interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "\n",
    "def load_ground_truth(img_path, labels_folder, img_w, img_h):\n",
    "    filename = os.path.basename(img_path)\n",
    "    txt_name = os.path.splitext(filename)[0] + \".txt\"\n",
    "    txt_path = os.path.join(labels_folder, txt_name)\n",
    "    \n",
    "    gt_boxes = []\n",
    "    if os.path.exists(txt_path):\n",
    "        with open(txt_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = list(map(float, line.strip().split()))\n",
    "                if len(parts) >= 5:\n",
    "                    cls, cx, cy, bw, bh = parts[:5]\n",
    "                    l = int((cx - bw/2) * img_w)\n",
    "                    t = int((cy - bh/2) * img_h)\n",
    "                    r = int((cx + bw/2) * img_w)\n",
    "                    b = int((cy + bh/2) * img_h)\n",
    "                    gt_boxes.append([l, t, r, b, int(cls)])\n",
    "    return gt_boxes\n",
    "\n",
    "# --- CARGAR MODELO ---\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov8',\n",
    "    model_path=model_path,\n",
    "    confidence_threshold=confidence,\n",
    "    device='cuda:0' \n",
    ")\n",
    "\n",
    "# --- PROCESO ---\n",
    "img_paths = glob.glob(os.path.join(input_dir, \"*.jpg\"))\n",
    "saved_count = 0\n",
    "\n",
    "for img_file in tqdm(img_paths, desc=\"Buscando objetos no etiquetados\"):\n",
    "    \n",
    "    img_raw = cv2.imread(img_file)\n",
    "    if img_raw is None: continue\n",
    "    h, w, _ = img_raw.shape\n",
    "    \n",
    "    # 1. Cargar GT\n",
    "    gt_list = load_ground_truth(img_file, labels_dir, w, h)\n",
    "    \n",
    "    # 2. Predicciones SAHI\n",
    "    result_sahi = get_sliced_prediction(\n",
    "        img_file,\n",
    "        detection_model,\n",
    "        slice_height=slice_h,\n",
    "        slice_width=slice_w,\n",
    "        overlap_height_ratio=overlap_h,\n",
    "        overlap_width_ratio=overlap_w,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # 3. FILTRADO\n",
    "    filtered_predictions = []\n",
    "    original_preds = result_sahi.object_prediction_list\n",
    "    \n",
    "    for pred in original_preds:\n",
    "    \n",
    "        # Esto devuelve una lista [min_x, min_y, max_x, max_y]\n",
    "        pred_box = pred.bbox.to_xyxy() \n",
    "        \n",
    "        is_redundant = False\n",
    "        \n",
    "        # Comparar con cada caja del Ground Truth\n",
    "        for gt in gt_list:\n",
    "            gt_box = gt[:4] # [x1, y1, x2, y2]\n",
    "            \n",
    "            # Calculamos IoU\n",
    "            iou = get_iou(pred_box, gt_box)\n",
    "            \n",
    "            # Si se solapan mucho, marcamos como redundante (borrar)\n",
    "            # Ignoramos la clase (gt[4]) como pediste\n",
    "            if iou > iou_threshold_exclusion:\n",
    "                is_redundant = True\n",
    "                break \n",
    "        \n",
    "        if not is_redundant:\n",
    "            filtered_predictions.append(pred)\n",
    "\n",
    "    # 4. CONDICI√ìN DE GUARDADO: Solo si quedan cajas nuevas\n",
    "    if len(filtered_predictions) > 0:\n",
    "        \n",
    "        # Pintar\n",
    "        visualization_result = visualize_object_predictions(\n",
    "            image=img_raw, \n",
    "            object_prediction_list=filtered_predictions,\n",
    "            rect_th=2,\n",
    "            text_size=0.4,\n",
    "            text_th=1\n",
    "        )\n",
    "        img_result = visualization_result[\"image\"]\n",
    "        img_result_bgr = cv2.cvtColor(img_result, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Texto indicativo\n",
    "        cv2.putText(img_result_bgr, f\"Nuevos encontrados: {len(filtered_predictions)}\", \n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "        # Guardar\n",
    "        filename = os.path.basename(img_file)\n",
    "        save_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(save_path, img_result_bgr)\n",
    "        saved_count += 1\n",
    "\n",
    "print(\"\\n--- FINALIZADO ---\")\n",
    "print(f\"Total im√°genes procesadas: {len(img_paths)}\")\n",
    "print(f\"Im√°genes con nuevas detecciones guardadas: {saved_count}\")\n",
    "print(f\"Carpeta: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4717ea-f8fd-4cbe-85c2-de26206b9861",
   "metadata": {},
   "source": [
    "Ahora vamos a ver el histograma y decidimos que vamos a seleccionar unicamente las cajas que tienen una confianza igual o mayor a 0.75. COmentar que hemos elegido de threshold 0.75 para que nos pueda detectar los coches de los laterales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e8a5c-0505-49cf-b0cf-dcbd8ed7a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codigo 4 \n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Importaciones de SAHI\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "\n",
    "# --- CONFIGURACI√ìN CR√çTICA ---\n",
    "# Ajusta esto bas√°ndote en el histograma que obtengas\n",
    "CONFIDENCE_MINIMA_PARA_ACEPTAR = 0.75  # <--- ¬°CAMBIA ESTO LUEGO!\n",
    "\n",
    "# Configuraci√≥n del modelo y rutas\n",
    "tipo_modelo = 'kfold_distorted'\n",
    "num_fold = 2\n",
    "confidence_model = 0.5 # Umbral bajo para que SAHI detecte, luego filtramos nosotros arriba\n",
    "\n",
    "# RUTAS (Usa absolutas para evitar problemas)\n",
    "BASE_DIR = \"/home/alumno/Desktop/datos/Computer_vision/final_project\"\n",
    "INPUT_IMGS = os.path.join(BASE_DIR, \"UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/images/\")\n",
    "INPUT_LABELS = os.path.join(BASE_DIR, \"UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/train/labels/\")\n",
    "MODEL_PATH = os.path.join(BASE_DIR, f\"runs/{tipo_modelo}/fold_{num_fold}/weights/best.pt\")\n",
    "\n",
    "# Parametros SAHI y Filtrado\n",
    "slice_h, slice_w = 160, 160\n",
    "overlap_h, overlap_w = 0.2, 0.2\n",
    "IOU_DUPLICADOS = 0.30 # Si se solapa m√°s de esto con una etiqueta real, NO la a√±adimos\n",
    "\n",
    "# --- 1. HACER BACKUP DE SEGURIDAD (VITAL) ---\n",
    "backup_dir = INPUT_LABELS + \"_BACKUP_ORIGINAL\"\n",
    "if not os.path.exists(backup_dir):\n",
    "    print(f\"Creando copia de seguridad de las etiquetas en: {backup_dir}\")\n",
    "    shutil.copytree(INPUT_LABELS, backup_dir)\n",
    "else:\n",
    "    print(f\"Ya existe una copia de seguridad en: {backup_dir}. Seguimos.\")\n",
    "\n",
    "# --- FUNCIONES ---\n",
    "\n",
    "def get_iou(boxA, boxB):\n",
    "    # box format: [x1, y1, x2, y2]\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    return interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "\n",
    "def xyxy_to_yolo(box, img_w, img_h):\n",
    "    \"\"\"Convierte [x1, y1, x2, y2] a formato YOLO [x_center, y_center, width, height] normalizado\"\"\"\n",
    "    x1, y1, x2, y2 = box\n",
    "    dw = 1. / img_w\n",
    "    dh = 1. / img_h\n",
    "    x = (x1 + x2) / 2.0\n",
    "    y = (y1 + y2) / 2.0\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    \n",
    "    # Normalizar\n",
    "    x = x * dw\n",
    "    w = w * dw\n",
    "    y = y * dh\n",
    "    h = h * dh\n",
    "    return x, y, w, h\n",
    "\n",
    "def load_ground_truth(txt_path, img_w, img_h):\n",
    "    gt_boxes = []\n",
    "    if os.path.exists(txt_path):\n",
    "        with open(txt_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = list(map(float, line.strip().split()))\n",
    "                if len(parts) >= 5:\n",
    "                    cls, cx, cy, bw, bh = parts[:5]\n",
    "                    l = int((cx - bw/2) * img_w)\n",
    "                    t = int((cy - bh/2) * img_h)\n",
    "                    r = int((cx + bw/2) * img_w)\n",
    "                    b = int((cy + bh/2) * img_h)\n",
    "                    gt_boxes.append([l, t, r, b, int(cls)])\n",
    "    return gt_boxes\n",
    "\n",
    "# --- CARGAR MODELO ---\n",
    "print(\"Cargando modelo...\")\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov8',\n",
    "    model_path=MODEL_PATH,\n",
    "    confidence_threshold=confidence_model,\n",
    "    device='cuda:0'\n",
    ")\n",
    "\n",
    "# --- PROCESO PRINCIPAL ---\n",
    "img_paths = glob.glob(os.path.join(INPUT_IMGS, \"*.jpg\"))\n",
    "total_added = 0\n",
    "\n",
    "print(f\"Comenzando actualizaci√≥n de {len(img_paths)} archivos...\")\n",
    "print(f\"Criterio: Confianza > {CONFIDENCE_MINIMA_PARA_ACEPTAR} y NO solaparse con GT.\")\n",
    "\n",
    "for img_file in tqdm(img_paths, desc=\"Actualizando Labels\"):\n",
    "    # Rutas y dimensiones\n",
    "    filename = os.path.basename(img_file)\n",
    "    txt_name = os.path.splitext(filename)[0] + \".txt\"\n",
    "    txt_path = os.path.join(INPUT_LABELS, txt_name)\n",
    "    \n",
    "    img_raw = cv2.imread(img_file)\n",
    "    if img_raw is None: continue\n",
    "    h, w, _ = img_raw.shape\n",
    "    \n",
    "    # 1. Cargar etiquetas existentes\n",
    "    gt_boxes = load_ground_truth(txt_path, w, h)\n",
    "    \n",
    "    # 2. Obtener nuevas predicciones\n",
    "    result_sahi = get_sliced_prediction(\n",
    "        img_file,\n",
    "        detection_model,\n",
    "        slice_height=slice_h,\n",
    "        slice_width=slice_w,\n",
    "        overlap_height_ratio=overlap_h,\n",
    "        overlap_width_ratio=overlap_w,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    new_lines_to_add = []\n",
    "    \n",
    "    # 3. Filtrar y Preparar\n",
    "    for pred in result_sahi.object_prediction_list:\n",
    "        conf = pred.score.value\n",
    "        \n",
    "        # FILTRO 1: ¬øEs lo suficientemente fiable?\n",
    "        if conf < CONFIDENCE_MINIMA_PARA_ACEPTAR:\n",
    "            continue\n",
    "            \n",
    "        # Coordenadas y Clase\n",
    "        pred_box_xyxy = pred.bbox.to_xyxy()\n",
    "        cls_id = pred.category.id\n",
    "        \n",
    "        # FILTRO 2: ¬øYa existe en el GT? (Duplicados)\n",
    "        is_duplicate = False\n",
    "        for gt in gt_boxes:\n",
    "            gt_box_xyxy = gt[:4]\n",
    "            if get_iou(pred_box_xyxy, gt_box_xyxy) > IOU_DUPLICADOS:\n",
    "                is_duplicate = True\n",
    "                break\n",
    "        \n",
    "        if is_duplicate:\n",
    "            continue\n",
    "            \n",
    "        # ¬°ES NUEVA Y FIABLE! -> Convertir a formato YOLO para guardar\n",
    "        x_c, y_c, bw, bh = xyxy_to_yolo(pred_box_xyxy, w, h)\n",
    "        \n",
    "        # Formato string: class x_center y_center width height\n",
    "        line_str = f\"{int(cls_id)} {x_c:.6f} {y_c:.6f} {bw:.6f} {bh:.6f}\\n\"\n",
    "        new_lines_to_add.append(line_str)\n",
    "        \n",
    "    # 4. Escribir en el archivo (Append)\n",
    "    if new_lines_to_add:\n",
    "        # Abrimos en modo 'a' (append) para a√±adir al final\n",
    "        with open(txt_path, 'a') as f:\n",
    "            for line in new_lines_to_add:\n",
    "                f.write(line)\n",
    "        total_added += len(new_lines_to_add)\n",
    "\n",
    "print(\"\\n--- PROCESO TERMINADO ---\")\n",
    "print(f\"Se han a√±adido {total_added} nuevas etiquetas al dataset.\")\n",
    "print(f\"Tus etiquetas originales est√°n a salvo en: {backup_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c7b4f2-8002-42b0-b30c-47b6928dfba6",
   "metadata": {},
   "source": [
    "https://www.youtube.com/@TrafficCamWatch/videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c94257-0438-411e-91a8-70da1a221cbc",
   "metadata": {},
   "source": [
    "Cosas que quedan para hacer:\n",
    "- D Rise de lo que hemos obtenido\n",
    "- Sacar todos los resultados del test y graficos como matriz de confusi√≥n\n",
    "- Usar el video para ver si predice el coche accidentado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc01998-a992-425a-9f76-c5ec1ab5dc5e",
   "metadata": {},
   "source": [
    "Lo que vamos ha hacer es probar el mejor modelo obtenido hasta pero sin coger los mejores pesos. Tambi√©n como tenemos 9316 imagenes de entrenamiento. Quiero sacar mismos porcentajes de entrenamiento y validaci√≥n que hemos usado hasta ahora como tenemos k = 5 vamos a usar un 80% training y 20 % validation. Vamos a hacer la separaci√≥n estratificada nuevamente ya que las proporciones han podido cambiar por lo tanto es necesario hacer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c4c22-1b48-43dc-8498-a7a0c92434d2",
   "metadata": {},
   "source": [
    "Al aplicar SAHI se han obtenido nuevos objetos detectados por lo tanto, es necesario hacer un nuevo mapa de prioridades de los datos por si hubieran cambiado las proporciones de los nuevos datos. Ahora vamos a probar el mejor modelo obtenido pero sin coger los mejores pesos. Adem√°s, si solo ejecutamos el modelo con una partici√≥n simple (80/20), el 20% de tus datos (y por tanto, el 20% de las nuevas etiquetas que SAHI ha encontrado) se ir√°n a Validaci√≥n. El modelo nunca entrenar√° con ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995910ae-9f3f-4e2f-9a59-b3ed89f08e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GENERANDO 5 FOLDS ESTRATIFICADOS ---\n",
      "Dataset: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8\n",
      "Im√°genes cargadas para Training: 9316\n",
      "Im√°genes encontradas: 9316\n",
      "Analizando etiquetas...\n",
      "--- Distribuci√≥n de Prioridades ---\n",
      "priority_group\n",
      "0    1416\n",
      "1    5419\n",
      "2    2481\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Guardando configuraci√≥n en: /home/alumno/Desktop/datos/Computer_vision/final_project/folds_final_sahi\n",
      "\n",
      "‚úÖ ¬°Proceso completado! Tienes 5 archivos YAML listos para entrenar.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# ==========================================\n",
    "#        CONFIGURACI√ìN\n",
    "# ==========================================\n",
    "\n",
    "# 1. RUTA DEL DATASET ACTUALIZADO (Donde SAHI escribi√≥ los cambios)\n",
    "BASE_PROJECT_DIR = \"/home/alumno/Desktop/datos/Computer_vision/final_project\"\n",
    "dataset_path = os.path.join(BASE_PROJECT_DIR, \"UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8\")\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "# 2. TUS CLASES\n",
    "class_names = {0: 'bus', 1: 'car', 2: 'truck', 3: 'van'}\n",
    "\n",
    "# 3. MAPA DE PRIORIDADES (Para equilibrar clases dif√≠ciles)\n",
    "priority_map = {\n",
    "    0: 0,  # Bus    -> Prioridad M√ÅXIMA\n",
    "    2: 1,  # Truck  -> Prioridad MEDIA\n",
    "    3: 1,  # Van    -> Prioridad MEDIA\n",
    "    1: 2   # Car    -> Prioridad BAJA\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "#           LOGICA DEL PROGRAMA\n",
    "# ==========================================\n",
    "\n",
    "print(f\"--- GENERANDO {num_folds} FOLDS ESTRATIFICADOS ---\")\n",
    "print(f\"Dataset: {dataset_path}\")\n",
    "\n",
    "# 1. Buscar im√°genes (Definimos espec√≠ficamente la carpeta de entrenamiento)\n",
    "train_dir = os.path.join(dataset_path, \"train/images\")\n",
    "\n",
    "# Primero probamos b√∫squeda directa\n",
    "images = glob.glob(os.path.join(train_dir, '*.jpg'))\n",
    "\n",
    "# Si no encuentra nada, buscamos recursivamente PERO SOLO dentro de train\n",
    "if len(images) == 0:\n",
    "    print(\"Buscando recursivamente DENTRO de train/images...\")\n",
    "    images = glob.glob(os.path.join(train_dir, '**', '*.jpg'), recursive=True)\n",
    "\n",
    "print(f\"Im√°genes cargadas para Training: {len(images)}\")\n",
    "\n",
    "print(f\"Im√°genes encontradas: {len(images)}\")\n",
    "\n",
    "data = []\n",
    "\n",
    "# 2. Calcular prioridad de cada imagen (leyendo los txt modificados)\n",
    "print(\"Analizando etiquetas...\")\n",
    "for img_path in images:\n",
    "    label_path = img_path.replace('/images/', '/labels/').replace('.jpg', '.txt')\n",
    "    assigned_priority = 99 \n",
    "    \n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        if lines:\n",
    "            try:\n",
    "                classes_in_img = set([int(line.split()[0]) for line in lines])\n",
    "            except:\n",
    "                classes_in_img = set()\n",
    "            \n",
    "            best_prio = 99\n",
    "            for c in classes_in_img:\n",
    "                p = priority_map.get(c, 10) \n",
    "                if p < best_prio:\n",
    "                    best_prio = p\n",
    "            assigned_priority = best_prio\n",
    "        else:\n",
    "             assigned_priority = 99\n",
    "    else:\n",
    "        assigned_priority = 99\n",
    "\n",
    "    data.append({\n",
    "        'path': os.path.abspath(img_path), \n",
    "        'priority_group': assigned_priority\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"--- Distribuci√≥n de Prioridades ---\")\n",
    "print(df['priority_group'].value_counts().sort_index())\n",
    "\n",
    "# 3. Generar los Folds\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Carpeta de salida para los yamls\n",
    "folds_output_dir = os.path.join(BASE_PROJECT_DIR, \"folds_final_sahi\")\n",
    "os.makedirs(folds_output_dir, exist_ok=True)\n",
    "\n",
    "X = df['path']\n",
    "y = df['priority_group']\n",
    "\n",
    "print(f\"\\nGuardando configuraci√≥n en: {folds_output_dir}\")\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    # Obtener listas de archivos\n",
    "    train_files = df.iloc[train_idx]['path'].tolist()\n",
    "    val_files = df.iloc[val_idx]['path'].tolist()\n",
    "    \n",
    "    # Rutas para los txt de lista\n",
    "    train_txt = os.path.join(folds_output_dir, f'train_fold_{fold_idx}.txt')\n",
    "    val_txt = os.path.join(folds_output_dir, f'val_fold_{fold_idx}.txt')\n",
    "    \n",
    "    # Escribir txts\n",
    "    with open(train_txt, 'w') as f:\n",
    "        f.write('\\n'.join(train_files))\n",
    "    with open(val_txt, 'w') as f:\n",
    "        f.write('\\n'.join(val_files))\n",
    "        \n",
    "    # Crear YAML\n",
    "    yaml_content = {\n",
    "        'path': dataset_path, # Ruta base (opcional si usas rutas absolutas en txt)\n",
    "        'train': train_txt,\n",
    "        'val': val_txt,\n",
    "        'names': class_names\n",
    "    }\n",
    "    \n",
    "    yaml_filename = os.path.join(folds_output_dir, f'fold_{fold_idx}.yaml')\n",
    "    with open(yaml_filename, 'w') as f:\n",
    "        yaml.dump(yaml_content, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n‚úÖ ¬°Proceso completado! Tienes 5 archivos YAML listos para entrenar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157962e-665e-4951-aff5-0eb9f1fef1a9",
   "metadata": {},
   "source": [
    "Se obtienen mismas proporciones esto tiene sentido ya que para que cambiaran las proporciones se deber√≠a obterner un van o truck en una foto con solo coches o que en una foto de las nuevas etiquetas se haya detectado un autobus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a8228f0-e1e3-40b8-948b-d7d8ece4abca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ants prop: 9316\n"
     ]
    }
   ],
   "source": [
    "print (f'Ants prop: {1350 + 5063 +2903}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "177195e5-9785-4711-bb52-099626a1452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despues prop: 9316\n",
      "El n√∫mero de prioridades de las clases ha cambiado as√≠: \n",
      "N√∫mero de buses en 66,\n",
      "N√∫mero de truck o van es 356,\n",
      "N√∫mero de prioridades de coche es -422\n"
     ]
    }
   ],
   "source": [
    "print (f'Despues prop: {1416 + 5419 + 2481 }')\n",
    "dif_bus = 1416-1350\n",
    "dif_truck_van = 5419 - 5063\n",
    "dif_car = 2481 - 2903\n",
    "print (f'El n√∫mero de prioridades de las clases ha cambiado as√≠: \\nN√∫mero de buses en {dif_bus},\\nN√∫mero de truck o van es {dif_truck_van},\\nN√∫mero de prioridades de coche es {dif_car}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7443152c-1d70-4dcf-a179-a79e1b77dc58",
   "metadata": {},
   "source": [
    "Comprobamos si realmente se han a√±adido las etiquetas correctamente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064842df-97c9-4d8c-a27b-ed9fabc6fe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de archivos: 9316\n",
      "TOTAL DE ETIQUETAS (CAJAS): 109505\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ==========================================\n",
    "#        CONFIGURACI√ìN\n",
    "# ==========================================\n",
    "\n",
    "# 1. Rutas\n",
    "# IMPORTANTE: Aqu√≠ ponemos la carpeta que gener√≥ tu script de K-Fold anterior\n",
    "folds_dir = 'folds_final_sahi' \n",
    "project_name = 'runs/kfold_final_sahi' # Nombre nuevo para no mezclar con pruebas anteriores\n",
    "model_name = \"yolov8n.pt\"\n",
    "\n",
    "# 2. Hiperpar√°metros (Mismos que con k fold distortioned)\n",
    "num_folds = 5  \n",
    "epochs = 10       \n",
    "img_size = 640\n",
    "batch_size = 16\n",
    "\n",
    "# 3. Transformaciones (Albumentations)\n",
    "custom_transforms = [\n",
    "    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "    A.ElasticTransform(alpha=1, sigma=50, p=0.5),\n",
    "    A.GaussNoise(p=0.3),\n",
    "]\n",
    "\n",
    "# Lista para guardar m√©tricas finales\n",
    "results_summary = [] \n",
    "\n",
    "# ==========================================\n",
    "#        BUCLE DE ENTRENAMIENTO\n",
    "# ==========================================\n",
    "\n",
    "print(f\"--- INICIANDO ENTRENAMIENTO FINAL (SAHI) ---\")\n",
    "print(f\"Leyendo Folds desde: {folds_dir}\")\n",
    "print(f\"Guardando resultados en: {project_name}\")\n",
    "\n",
    "if not os.path.exists(folds_dir):\n",
    "    print(f\"‚ùå ERROR CR√çTICO: No existe la carpeta '{folds_dir}'.\")\n",
    "    print(\"Aseg√∫rate de haber ejecutado el script de generaci√≥n de Folds primero.\")\n",
    "    exit()\n",
    "\n",
    "for k in range(num_folds):\n",
    "    yaml_file = os.path.join(folds_dir, f'fold_{k}.yaml')\n",
    "    run_name = f'fold_{k}'\n",
    "    \n",
    "    if not os.path.exists(yaml_file): \n",
    "        print(f\"‚ö†Ô∏è Aviso: No se encuentra {yaml_file}, saltando...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nTraining Fold {k} / {num_folds - 1}...\")\n",
    "    \n",
    "    # 1. Cargar modelo base limpio\n",
    "    model = YOLO(model_name)\n",
    "    \n",
    "    # 2. Entrenar\n",
    "    try:\n",
    "        model.train(\n",
    "            data=yaml_file,\n",
    "            epochs=epochs,\n",
    "            imgsz=img_size,\n",
    "            batch=batch_size,\n",
    "            project=project_name,\n",
    "            name=run_name,\n",
    "            patience=10, \n",
    "            verbose=True,\n",
    "            exist_ok=True,\n",
    "            augmentations=custom_transforms, \n",
    "            seed=42,\n",
    "            deterministic=True,\n",
    "            workers=4,\n",
    "            degrees=0.0,\n",
    "            mosaic=0.0,      \n",
    "            mixup=0.0,        \n",
    "            copy_paste=0.0   \n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error entrenando Fold {k}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 3. Extraer m√©tricas del CSV generado\n",
    "    results_path = os.path.join(project_name, run_name, 'results.csv')\n",
    "    \n",
    "    if os.path.exists(results_path):\n",
    "        df = pd.read_csv(results_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # Mejor √©poca seg√∫n mAP50\n",
    "        best_idx = df['metrics/mAP50(B)'].idxmax()\n",
    "        best_epoch_data = df.iloc[best_idx]\n",
    "        \n",
    "        results_summary.append({\n",
    "            'Fold': k,\n",
    "            'Precision': best_epoch_data['metrics/precision(B)'],\n",
    "            'Recall': best_epoch_data['metrics/recall(B)'],\n",
    "            'mAP@50': best_epoch_data['metrics/mAP50(B)'],\n",
    "            'mAP@50-95': best_epoch_data['metrics/mAP50-95(B)']\n",
    "        })\n",
    "        \n",
    "        print(f\"-> Fold {k}: Mejor mAP@50: {best_epoch_data['metrics/mAP50(B)']:.4f}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No se encontr√≥ CSV de resultados para el Fold {k}\")\n",
    "\n",
    "# ==========================================\n",
    "#        RESUMEN FINAL\n",
    "# ==========================================\n",
    "\n",
    "if results_summary:\n",
    "    df_final = pd.DataFrame(results_summary)\n",
    "    df_final.set_index('Fold', inplace=True)\n",
    "\n",
    "    # Calcular promedio\n",
    "    df_final.loc['PROMEDIO'] = df_final.mean()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RESUMEN DE RESULTADOS (SAHI + MEJORES HIPERPAR√ÅMETROS)\")\n",
    "    print(\"=\"*50)\n",
    "    print(df_final)\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Guardar CSV final\n",
    "    output_csv_name = 'resultados_finales_sahi.csv'\n",
    "    df_final.to_csv(output_csv_name)\n",
    "    print(f\"‚úÖ Resultados guardados en '{output_csv_name}'\")\n",
    "else:\n",
    "    print(\"‚ùå No se generaron resultados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d126110-d3c1-49d5-ac5a-4f5233667734",
   "metadata": {},
   "source": [
    "Resultado del modelo anterior\n",
    "\n",
    "==================================================\n",
    "RESUMEN DE RESULTADOS (SAHI + MEJORES HIPERPAR√ÅMETROS)\n",
    "==================================================\n",
    "          Precision    Recall    mAP@50  mAP@50-95\n",
    "Fold\n",
    "0          0.901190  0.848270  0.916710   0.762570\n",
    "1          0.911780  0.841870  0.918840   0.764560\n",
    "2          0.907280  0.837970  0.918900   0.759570\n",
    "3          0.908530  0.832250  0.912770   0.746370\n",
    "4          0.909110  0.837330  0.911710   0.754460\n",
    "PROMEDIO   0.907578  0.839538  0.915786   0.757506\n",
    "==================================================\n",
    "‚úÖ Resultados guardados en 'resultados_finales_sahi.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c11814-674c-4ced-a00b-597c81f0d4ed",
   "metadata": {},
   "source": [
    "Evaluamos el primer modelo sin distortioned con el mejor modelo obtenido con distortioned tras haber aplicado sahi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0016367-2132-4d33-b445-82746006b729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ü•ä INICIANDO LA BATALLA DE MODELOS ü•ä ---\n",
      "\n",
      "1Ô∏è‚É£ Evaluando Modelo ANTIGUO (Sin SAHI)...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.2¬±0.0 ms, read: 5.1¬±6.9 MB/s, size: 49.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/valid/labels... 500 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 500/500 463.8it/s 1.1s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/valid/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 32/32 10.8it/s 3.0s0.1s\n",
      "                   all        500       4845      0.927      0.892      0.949      0.818\n",
      "Speed: 0.8ms preprocess, 0.6ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val80\u001b[0m\n",
      "\n",
      "2Ô∏è‚É£ Evaluando Modelo NUEVO (Con SAHI)...\n",
      "Ultralytics 8.3.247 üöÄ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, 97248MiB)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1261.2¬±349.0 MB/s, size: 53.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/alumno/Desktop/datos/Computer_vision/final_project/UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/valid/labels.cache... 500 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 500/500 906.3Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 32/32 9.2it/s 3.5s<0.1s\n",
      "                   all        500       4845      0.856      0.845      0.909       0.77\n",
      "Speed: 1.0ms preprocess, 0.5ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1m/home/alumno/Desktop/datos/Computer_vision/final_project/runs/detect/val81\u001b[0m\n",
      "\n",
      "==================================================\n",
      "üìä RESULTADOS FINALES EN TEST (500 Im√°genes)\n",
      "==================================================\n",
      "Modelo               | mAP@50     | mAP@50-95 \n",
      "----------------------------------------------\n",
      "ANTIGUO (Pre-SAHI)   | 0.9485     | 0.8182\n",
      "NUEVO (Con SAHI)     | 0.9094     | 0.7705\n",
      "----------------------------------------------\n",
      "‚ö†Ô∏è El modelo antiguo gana por 0.0391 (Revisar Falsos Positivos)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# --- CONFIGURACI√ìN ---\n",
    "BASE_DIR = \"/home/alumno/Desktop/datos/Computer_vision/final_project\"\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8\")\n",
    "\n",
    "# Rutas de tus campeones\n",
    "path_old = os.path.join(BASE_DIR, \"runs/kfold/fold_3/weights/best.pt\")\n",
    "path_new = os.path.join(BASE_DIR, \"runs/kfold_final_sahi/fold_1/weights/best.pt\")\n",
    "\n",
    "# Creamos un YAML temporal que apunte al TEST REAL (Valid folder)\n",
    "yaml_test_path = \"config_comparativa.yaml\"\n",
    "yaml_content = {\n",
    "    'path': DATASET_DIR,\n",
    "    'train': \"train/images\", # Irrelevante para val\n",
    "    'val': \"valid/images\",   # <--- LA CLAVE: Usamos las 500 im√°genes limpias\n",
    "    'names': {0: 'bus', 1: 'car', 2: 'truck', 3: 'van'}\n",
    "}\n",
    "with open(yaml_test_path, 'w') as f:\n",
    "    yaml.dump(yaml_content, f)\n",
    "\n",
    "print(\"--- ü•ä INICIANDO LA BATALLA DE MODELOS ü•ä ---\")\n",
    "\n",
    "# --- EVALUAR MODELO ANTIGUO ---\n",
    "if os.path.exists(path_old):\n",
    "    print(f\"\\n1Ô∏è‚É£ Evaluando Modelo ANTIGUO (Sin SAHI)...\")\n",
    "    model_old = YOLO(path_old)\n",
    "    metrics_old = model_old.val(data=yaml_test_path, split='val', verbose=False)\n",
    "    map50_old = metrics_old.box.map50\n",
    "    map5095_old = metrics_old.box.map\n",
    "else:\n",
    "    print(\"‚ùå No encuentro el modelo antiguo.\")\n",
    "    map50_old = 0\n",
    "\n",
    "# --- EVALUAR MODELO NUEVO ---\n",
    "if os.path.exists(path_new):\n",
    "    print(f\"\\n2Ô∏è‚É£ Evaluando Modelo NUEVO (Con SAHI)...\")\n",
    "    model_new = YOLO(path_new)\n",
    "    metrics_new = model_new.val(data=yaml_test_path, split='val', verbose=False)\n",
    "    map50_new = metrics_new.box.map50\n",
    "    map5095_new = metrics_new.box.map\n",
    "else:\n",
    "    print(\"‚ùå No encuentro el modelo nuevo.\")\n",
    "    map50_new = 0\n",
    "\n",
    "# --- RESULTADOS FINALES ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"üìä RESULTADOS FINALES EN TEST (500 Im√°genes)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Modelo':<20} | {'mAP@50':<10} | {'mAP@50-95':<10}\")\n",
    "print(\"-\" * 46)\n",
    "print(f\"{'ANTIGUO (Pre-SAHI)':<20} | {map50_old:.4f}     | {map5095_old:.4f}\")\n",
    "print(f\"{'NUEVO (Con SAHI)':<20} | {map50_new:.4f}     | {map5095_new:.4f}\")\n",
    "print(\"-\" * 46)\n",
    "\n",
    "diff = map50_new - map50_old\n",
    "if diff > 0:\n",
    "    print(f\"üéâ ¬°√âXITO! El modelo con SAHI ha mejorado el mAP en +{diff:.4f}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è El modelo antiguo gana por {abs(diff):.4f} (Revisar Falsos Positivos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d564ab14-55ff-49a2-ae6b-af97a247905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelos...\n",
      "‚úÖ Imagen guardada como 'comparativa_visual_final.png'\n",
      "√Åbrela para ver las diferencias.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# --- CONFIGURACI√ìN ---\n",
    "BASE_DIR = \"/home/alumno/Desktop/datos/Computer_vision/final_project\"\n",
    "IMG_DIR = os.path.join(BASE_DIR, \"UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/valid/images\")\n",
    "\n",
    "path_old = os.path.join(BASE_DIR, \"runs/kfold/fold_3/weights/best.pt\")\n",
    "path_new = os.path.join(BASE_DIR, \"runs/kfold_final_sahi/fold_1/weights/best.pt\")\n",
    "\n",
    "# --- CARGAR ---\n",
    "print(\"Cargando modelos...\")\n",
    "model_old = YOLO(path_old)\n",
    "model_new = YOLO(path_new)\n",
    "\n",
    "# Obtener todas las im√°genes de validaci√≥n\n",
    "all_images = glob.glob(os.path.join(IMG_DIR, \"*.jpg\"))\n",
    "\n",
    "if not all_images:\n",
    "    print(\"‚ùå No encuentro im√°genes en valid/images\")\n",
    "    exit()\n",
    "\n",
    "# Seleccionar 3 al azar\n",
    "selected_images = random.sample(all_images, 3)\n",
    "\n",
    "# --- VISUALIZAR ---\n",
    "# Configuramos el plot para mostrar 3 filas (una por imagen)\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "for i, img_path in enumerate(selected_images):\n",
    "    filename = os.path.basename(img_path)\n",
    "    \n",
    "    # Predecir\n",
    "    res_old = model_old.predict(img_path, conf=0.4, verbose=False) # Confianza normal\n",
    "    res_new = model_new.predict(img_path, conf=0.4, verbose=False)\n",
    "    \n",
    "    # Pintar\n",
    "    plot_old = res_old[0].plot() # BGR\n",
    "    plot_new = res_new[0].plot() # BGR\n",
    "    \n",
    "    # Convertir a RGB para matplotlib\n",
    "    plot_old = cv2.cvtColor(plot_old, cv2.COLOR_BGR2RGB)\n",
    "    plot_new = cv2.cvtColor(plot_new, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Subplot Izquierda (Viejo)\n",
    "    plt.subplot(3, 2, i*2 + 1)\n",
    "    if i == 0: plt.title(f\"ANTIGUO (Fold 3)\\n{filename}\", fontsize=14, color='red')\n",
    "    else: plt.title(f\"{filename}\", fontsize=10)\n",
    "    plt.imshow(plot_old)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Subplot Derecha (Nuevo)\n",
    "    plt.subplot(3, 2, i*2 + 2)\n",
    "    if i == 0: plt.title(f\"NUEVO (SAHI - Fold 1)\\n{filename}\", fontsize=14, color='green')\n",
    "    plt.imshow(plot_new)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Guardar en archivo para que puedas bajarlo\n",
    "plt.savefig(\"comparativa_visual_final.png\")\n",
    "print(\"‚úÖ Imagen guardada como 'comparativa_visual_final.png'\")\n",
    "print(\"√Åbrela para ver las diferencias.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3b3845d-6a3e-42c5-bf98-7a7bac0500e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelos...\n",
      "Buscando diferencias interesantes en 500 im√°genes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñé                                          | 3/500 [00:00<00:23, 20.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ¬°Encontradas 3 im√°genes conflictivas!\n",
      "üì∏ Imagen guardada como 'comparativa_inteligente.png'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURACI√ìN ---\n",
    "BASE_DIR = \"/home/alumno/Desktop/datos/Computer_vision/final_project\"\n",
    "IMG_DIR = os.path.join(BASE_DIR, \"UA-DETRAC-DATASET-10K.v2-2024-11-14-3-48pm.yolov8/valid/images\")\n",
    "\n",
    "path_old = os.path.join(BASE_DIR, \"runs/kfold/fold_3/weights/best.pt\")\n",
    "path_new = os.path.join(BASE_DIR, \"runs/kfold_final_sahi/fold_1/weights/best.pt\")\n",
    "\n",
    "# --- CARGAR MODELOS ---\n",
    "print(\"Cargando modelos...\")\n",
    "model_old = YOLO(path_old)\n",
    "model_new = YOLO(path_new)\n",
    "\n",
    "# Obtener im√°genes\n",
    "all_images = glob.glob(os.path.join(IMG_DIR, \"*.jpg\"))\n",
    "random.shuffle(all_images) # Mezclamos para no ver siempre las primeras\n",
    "\n",
    "print(f\"Buscando diferencias interesantes en {len(all_images)} im√°genes...\")\n",
    "\n",
    "# --- BUSCADOR DE INTERESANTES ---\n",
    "interesting_images = []\n",
    "target_count = 3 # Cu√°ntas im√°genes queremos encontrar\n",
    "\n",
    "for img_path in tqdm(all_images):\n",
    "    # Predicci√≥n r√°pida (solo para contar cajas)\n",
    "    res_old = model_old.predict(img_path, conf=0.5, verbose=False)\n",
    "    res_new = model_new.predict(img_path, conf=0.5, verbose=False)\n",
    "    \n",
    "    count_old = len(res_old[0].boxes)\n",
    "    count_new = len(res_new[0].boxes)\n",
    "    \n",
    "    # CRITERIO DE SELECCI√ìN:\n",
    "    # Queremos fotos donde el NUEVO vea M√ÅS coches que el viejo\n",
    "    # (Diferencia de al menos 2 coches para que sea evidente)\n",
    "    if count_new >= count_old + 2:\n",
    "        interesting_images.append(img_path)\n",
    "        if len(interesting_images) >= target_count:\n",
    "            break\n",
    "\n",
    "if not interesting_images:\n",
    "    print(\"No encontr√© diferencias grandes. Bajando exigencia...\")\n",
    "    # Si no encuentra con +2, coge cualquiera aleatoria\n",
    "    interesting_images = random.sample(all_images, 3)\n",
    "\n",
    "# --- VISUALIZAR ---\n",
    "print(f\"‚úÖ ¬°Encontradas {len(interesting_images)} im√°genes conflictivas!\")\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "for i, img_path in enumerate(interesting_images):\n",
    "    filename = os.path.basename(img_path)\n",
    "    \n",
    "    # Predecir y Pintar\n",
    "    res_old = model_old.predict(img_path, conf=0.5, verbose=False)\n",
    "    res_new = model_new.predict(img_path, conf=0.5, verbose=False)\n",
    "    \n",
    "    # T√≠tulos con el conteo\n",
    "    c_old = len(res_old[0].boxes)\n",
    "    c_new = len(res_new[0].boxes)\n",
    "    \n",
    "    plot_old = cv2.cvtColor(res_old[0].plot(), cv2.COLOR_BGR2RGB)\n",
    "    plot_new = cv2.cvtColor(res_new[0].plot(), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Subplot Izquierda\n",
    "    plt.subplot(3, 2, i*2 + 1)\n",
    "    plt.title(f\"ANTIGUO: {c_old} coches detected\", fontsize=12, color='red')\n",
    "    plt.imshow(plot_old)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Subplot Derecha\n",
    "    plt.subplot(3, 2, i*2 + 2)\n",
    "    plt.title(f\"NUEVO (SAHI): {c_new} coches detected\", fontsize=12, color='green')\n",
    "    plt.imshow(plot_new)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparativa_inteligente.png\")\n",
    "print(\"üì∏ Imagen guardada como 'comparativa_inteligente.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526927d-9158-435d-b00b-959b0c80656f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (ML Master)",
   "language": "python",
   "name": "py313ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
